{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_GAT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV-Of_mxEGQK",
        "outputId": "a6bd3016-b704-4aac-cd77-9c424c1b50fe"
      },
      "source": [
        "!pip install dgl-0.3-cp37-cp37m-manylinux1_x86_64.whl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./dgl-0.3-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl==0.3) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl==0.3) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl==0.3) (2.5.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.1->dgl==0.3) (4.4.2)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5oGzVfVEOGb",
        "outputId": "49da5830-fa7b-4cbd-d551-e029befdb74f"
      },
      "source": [
        "!pip uninstall -y networkx"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling networkx-2.5.1:\n",
            "  Successfully uninstalled networkx-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-hrz9BuERAs",
        "outputId": "1dc0e3be-4811-41a8-9ac9-bddffeed16bd"
      },
      "source": [
        "!pip install networkx-2.5-py3-none-any.whl"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./networkx-2.5-py3-none-any.whl\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx==2.5) (4.4.2)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: networkx\n",
            "Successfully installed networkx-2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otMpuMc7ESr9",
        "outputId": "e8eb7709-f8f3-4c96-aa29-2f7c81e10a87"
      },
      "source": [
        "!pip uninstall -y imgaug"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling imgaug-0.2.9:\n",
            "  Successfully uninstalled imgaug-0.2.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsnWPNtaEUU1",
        "outputId": "f3d85525-2c2f-42c5-9d79-e43c0f7ead19"
      },
      "source": [
        "!pip install imgaug-0.2.7-py3-none-any.whl"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./imgaug-0.2.7-py3-none-any.whl\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.7) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.7) (1.19.5)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.7) (1.7.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.7) (3.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.7) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.7) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.7) (1.4.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.7) (2.4.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.2.7) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.2.7) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.2.7) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.2.7) (1.3.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.7) (2.5)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.7) (1.1.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug==0.2.7) (4.4.2)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.7 which is incompatible.\u001b[0m\n",
            "Installing collected packages: imgaug\n",
            "Successfully installed imgaug-0.2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_ltbUZxEVzM",
        "outputId": "341b3841-6786-4ddc-8795-abda94565117"
      },
      "source": [
        "!pip install tensorboardX-2.1-py2.py3-none-any.whl"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./tensorboardX-2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.1) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.1) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.1) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX==2.1) (56.1.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwXlOGt3EXI8",
        "outputId": "73afdab6-b3bb-469e-b617-b3c61f9bf883"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WTpDIGfEZId"
      },
      "source": [
        "import os\n",
        "#path=\"/content/drive/MyDrive\"\n",
        "path=\"/content/drive/MyDrive/Others\"\n",
        "#path=\"/content/\"\n",
        "os.chdir(path)\n",
        "os.listdir(path)\n",
        "!python utils.py\n",
        "!python utils_data.py\n",
        "!python utils_layers.py"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBqnXFtMEa1t"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "\n",
        "import dgl.init\n",
        "import numpy as np\n",
        "import tensorboardX\n",
        "import torch as th\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import utils_data\n",
        "from utils_layers import GATNet"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7Lt81A4Eqq_"
      },
      "source": [
        "args={'model':'GAT_TwoLayers',\n",
        "  'dataset':'squirrel',\n",
        "  'num_hidden':48,\n",
        "  'num_heads_layer_one':8,\n",
        "  'num_heads_layer_two':1,\n",
        "  'dropout_rate':0.5,\n",
        "  'learning_rate':0.05,\n",
        "  'weight_decay_layer_one':5e-06,\n",
        "  'weight_decay_layer_two':5e-06,\n",
        "  'num_epochs_patience':100,\n",
        "  'num_epochs_max':5000,\n",
        "  'run_id':0,\n",
        "  'dataset_split':'splits/squirrel_split_0.6_0.2_0.npz',\n",
        "  'learning_rate_decay_patience':50,\n",
        "  'learning_rate_decay_factor':0.8\n",
        "   }\n",
        "#python train_GAT.py --dataset cora --num_hidden 8 --num_heads_layer_one 8 --num_heads_layer_two 1 --weight_decay_layer_one 5e-06 --weight_decay_layer_two 5e-06 --learning_rate 0.05 --dropout_rate 0.5 --run_id 0 --dataset_split splits/cora_split_0.6_0.2_0.npz"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwZ60xBNEonu",
        "outputId": "175dbd24-0dd5-46f2-e567-61c598d70310"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    record = pd.DataFrame(columns=['Train Loss', 'Val Loss', 'Train Acc', 'Val Acc'])\n",
        "\n",
        "    if args['dataset_split'] == 'jknet':\n",
        "        g, features, labels, train_mask, val_mask, test_mask, num_features, num_labels = utils_data.load_data(\n",
        "            args['dataset'], None, 0.6, 0.2)\n",
        "    else:\n",
        "        g, features, labels, train_mask, val_mask, test_mask, num_features, num_labels = utils_data.load_data(\n",
        "            args['dataset'], args['dataset_split'], None, None)\n",
        "\n",
        "    g.set_n_initializer(dgl.init.zero_initializer)\n",
        "    g.set_e_initializer(dgl.init.zero_initializer)\n",
        "\n",
        "    net = GATNet(num_input_features=num_features, num_output_classes=num_labels, num_hidden=args['num_hidden'],\n",
        "                 dropout_rate=args['dropout_rate'], num_heads_layer_one=args['num_heads_layer_one'], num_heads_layer_two=args['num_heads_layer_two'])\n",
        "\n",
        "    optimizer = th.optim.Adam([{'params': net.gat1.parameters(), 'weight_decay': args['weight_decay_layer_one']},\n",
        "                               {'params': net.gat2.parameters(), 'weight_decay': args['weight_decay_layer_two']}],\n",
        "                              lr=args['learning_rate'])\n",
        "    learning_rate_scheduler = th.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,factor=args['learning_rate_decay_factor'],patience=args['learning_rate_decay_patience'])\n",
        "    writer = tensorboardX.SummaryWriter(logdir=f'runs/{args[\"model\"]}_{args[\"run_id\"]}')\n",
        "\n",
        "    features = features.to(\"cpu\")\n",
        "    labels = labels.to(\"cpu\")\n",
        "    train_mask = train_mask.to(\"cpu\")\n",
        "    val_mask = val_mask.to(\"cpu\")\n",
        "    test_mask = test_mask.to(\"cpu\")\n",
        "\n",
        "    # Adapted from https://github.com/PetarV-/GAT/blob/master/execute_cora.py\n",
        "    patience = args['num_epochs_patience']\n",
        "    vlss_mn = np.inf\n",
        "    vacc_mx = 0.0\n",
        "    vacc_early_model = None\n",
        "    vlss_early_model = None\n",
        "    state_dict_early_model = None\n",
        "    curr_step = 0\n",
        "\n",
        "    # Adapted from https://docs.dgl.ai/tutorials/models/1_gnn/1_gcn.html\n",
        "    dur = []\n",
        "    test_time = 0.0\n",
        "    for epoch in range(500):\n",
        "        t0 = time.time()\n",
        "\n",
        "        net.train()\n",
        "        train_logits = net(g, features)\n",
        "        train_logp = F.log_softmax(train_logits, 1)\n",
        "        train_loss = F.nll_loss(train_logp[train_mask], labels[train_mask])\n",
        "        train_pred = train_logp.argmax(dim=1)\n",
        "        train_acc = th.eq(train_pred[train_mask], labels[train_mask]).float().mean().item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        net.eval()\n",
        "        with th.no_grad():\n",
        "            val_logits = net(g, features)\n",
        "            val_logp = F.log_softmax(val_logits, 1)\n",
        "            val_loss = F.nll_loss(val_logp[val_mask], labels[val_mask]).item()\n",
        "            val_pred = val_logp.argmax(dim=1)\n",
        "            val_acc = th.eq(val_pred[val_mask], labels[val_mask]).float().mean().item()\n",
        "\n",
        "        learning_rate_scheduler.step(val_loss)\n",
        "\n",
        "        dur.append(time.time() - t0)\n",
        "\n",
        "        print(\n",
        "            \"Epoch {:05d} | Train Loss {:.4f} | Train Acc {:.4f} | Val Loss {:.4f} | Val Acc {:.4f} | Time(s) {:.4f}\".format(\n",
        "                epoch, train_loss.item(), train_acc, val_loss, val_acc, sum(dur) / len(dur)))\n",
        "\n",
        "        writer.add_scalar('Train Loss', train_loss.item(), epoch)\n",
        "        writer.add_scalar('Val Loss', val_loss, epoch)\n",
        "        writer.add_scalar('Train Acc', train_acc, epoch)\n",
        "        writer.add_scalar('Val Acc', val_acc, epoch)\n",
        "\n",
        "        test_time += (sum(dur) / len(dur))\n",
        "        new={'Train Loss':train_loss.item(),'Val Loss':val_loss,'Train Acc':train_acc,'Val Acc':val_acc}\n",
        "\n",
        "        record=record.append(new,ignore_index=True)   # ignore_index=True,表示不按原来的索引，从0开始自动递增\n",
        "\n",
        "        # Adapted from https://github.com/PetarV-/GAT/blob/master/execute_cora.py\n",
        "        if val_acc >= vacc_mx or val_loss <= vlss_mn:\n",
        "            if val_acc >= vacc_mx and val_loss <= vlss_mn:\n",
        "                vacc_early_model = val_acc\n",
        "                vlss_early_model = val_loss\n",
        "                state_dict_early_model = net.state_dict()\n",
        "            vacc_mx = np.max((val_acc, vacc_mx))\n",
        "            vlss_mn = np.min((val_loss, vlss_mn))\n",
        "            curr_step = 0\n",
        "        else:\n",
        "            curr_step += 1\n",
        "            if curr_step >= patience:\n",
        "                print()\n",
        "#                break\n",
        "\n",
        "    record.to_csv('Test_Result/test_{model}_{dataset}_{rec_time}.csv'.format(model=args[\"model\"], dataset=args[\"dataset\"], rec_time=test_time))\n",
        "    print(test_time)\n",
        "\n",
        "    net.load_state_dict(state_dict_early_model)\n",
        "    net.eval()\n",
        "    with th.no_grad():\n",
        "        test_logits = net(g, features)\n",
        "        test_logp = F.log_softmax(test_logits, 1)\n",
        "        test_loss = F.nll_loss(test_logp[test_mask], labels[test_mask]).item()\n",
        "        test_pred = test_logp.argmax(dim=1)\n",
        "        test_acc = th.eq(test_pred[test_mask], labels[test_mask]).float().mean().item()\n",
        "        test_hidden_features = net.gat1(g, features).cpu().numpy()\n",
        "\n",
        "        final_train_pred = test_pred[train_mask].cpu().numpy()\n",
        "        final_val_pred = test_pred[val_mask].cpu().numpy()\n",
        "        final_test_pred = test_pred[test_mask].cpu().numpy()\n",
        "\n",
        "    '''\n",
        "    results_dict = dir(args)\n",
        "    results_dict['test_loss'] = test_loss\n",
        "    results_dict['test_acc'] = test_acc\n",
        "    results_dict['actual_epochs'] = 1 + epoch\n",
        "    results_dict['val_acc_max'] = vacc_mx\n",
        "    results_dict['val_loss_min'] = vlss_mn\n",
        "    results_dict['total_time'] = sum(dur)\n",
        "    '''\n",
        "\n",
        "    '''\n",
        "    with open(os.path.join('runs', f'{args.model}_{args.run_id}_results.txt'), 'w') as outfile:\n",
        "        outfile.write(json.dumps(results_dict) + '\\n')\n",
        "    np.savez_compressed(os.path.join('runs', f'{args.model}_{args.run_id}_hidden_features.npz'),\n",
        "                        hidden_features=test_hidden_features)\n",
        "    np.savez_compressed(os.path.join('runs', f'{args.model}_{args.run_id}_final_train_predictions.npz'),\n",
        "                        final_train_predictions=final_train_pred)\n",
        "    np.savez_compressed(os.path.join('runs', f'{args.model}_{args.run_id}_final_val_predictions.npz'),\n",
        "                        final_val_predictions=final_val_pred)\n",
        "    np.savez_compressed(os.path.join('runs', f'{args.model}_{args.run_id}_final_test_predictions.npz'),\n",
        "                        final_test_predictions=final_test_pred)\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test1\n",
            "test4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Others/utils.py:118: RuntimeWarning: divide by zero encountered in power\n",
            "  r_inv = np.power(rowsum, -1).flatten()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test5\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "DGLGraph(num_nodes=5201, num_edges=222134,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "<class 'dgl.graph.DGLGraph'>\n",
            "<class 'dgl.graph.DGLGraph'>\n",
            "test_end\n",
            "<class 'dgl.graph.DGLGraph'>\n",
            "Epoch 00000 | Train Loss 1.8425 | Train Acc 0.2007 | Val Loss 4.4573 | Val Acc 0.2007 | Time(s) 74.1971\n",
            "Epoch 00001 | Train Loss 6.0624 | Train Acc 0.1907 | Val Loss 2.0964 | Val Acc 0.2007 | Time(s) 76.1317\n",
            "Epoch 00002 | Train Loss 2.3845 | Train Acc 0.1975 | Val Loss 2.8878 | Val Acc 0.1947 | Time(s) 74.7912\n",
            "Epoch 00003 | Train Loss 3.5641 | Train Acc 0.2095 | Val Loss 2.2910 | Val Acc 0.1947 | Time(s) 74.0009\n",
            "Epoch 00004 | Train Loss 2.5014 | Train Acc 0.2059 | Val Loss 2.0392 | Val Acc 0.2043 | Time(s) 73.1615\n",
            "Epoch 00005 | Train Loss 2.3897 | Train Acc 0.2192 | Val Loss 1.9173 | Val Acc 0.2019 | Time(s) 72.2272\n",
            "Epoch 00006 | Train Loss 2.2861 | Train Acc 0.2344 | Val Loss 2.0084 | Val Acc 0.1929 | Time(s) 72.1651\n",
            "Epoch 00007 | Train Loss 2.4484 | Train Acc 0.2111 | Val Loss 1.9613 | Val Acc 0.2001 | Time(s) 72.9211\n",
            "Epoch 00008 | Train Loss 2.4216 | Train Acc 0.1895 | Val Loss 1.7568 | Val Acc 0.2013 | Time(s) 73.4914\n",
            "Epoch 00009 | Train Loss 2.0119 | Train Acc 0.2055 | Val Loss 1.7802 | Val Acc 0.1989 | Time(s) 73.5929\n",
            "Epoch 00010 | Train Loss 2.0092 | Train Acc 0.2159 | Val Loss 1.8786 | Val Acc 0.2007 | Time(s) 73.5896\n",
            "Epoch 00011 | Train Loss 2.0420 | Train Acc 0.2328 | Val Loss 1.8660 | Val Acc 0.2133 | Time(s) 72.5496\n",
            "Epoch 00012 | Train Loss 2.0483 | Train Acc 0.2368 | Val Loss 1.8108 | Val Acc 0.2236 | Time(s) 71.6989\n",
            "Epoch 00013 | Train Loss 2.0119 | Train Acc 0.2296 | Val Loss 1.7287 | Val Acc 0.2139 | Time(s) 70.9908\n",
            "Epoch 00014 | Train Loss 1.9028 | Train Acc 0.2272 | Val Loss 1.6529 | Val Acc 0.2248 | Time(s) 70.3870\n",
            "Epoch 00015 | Train Loss 1.7869 | Train Acc 0.2476 | Val Loss 1.6442 | Val Acc 0.2127 | Time(s) 70.0425\n",
            "Epoch 00016 | Train Loss 1.8038 | Train Acc 0.2488 | Val Loss 1.6715 | Val Acc 0.2368 | Time(s) 70.5583\n",
            "Epoch 00017 | Train Loss 1.8716 | Train Acc 0.2352 | Val Loss 1.6764 | Val Acc 0.2169 | Time(s) 71.1038\n",
            "Epoch 00018 | Train Loss 1.8678 | Train Acc 0.2224 | Val Loss 1.6424 | Val Acc 0.2200 | Time(s) 71.6394\n",
            "Epoch 00019 | Train Loss 1.7918 | Train Acc 0.2304 | Val Loss 1.6026 | Val Acc 0.2446 | Time(s) 72.1111\n",
            "Epoch 00020 | Train Loss 1.6827 | Train Acc 0.2492 | Val Loss 1.5907 | Val Acc 0.2674 | Time(s) 72.2500\n",
            "Epoch 00021 | Train Loss 1.6815 | Train Acc 0.2604 | Val Loss 1.6028 | Val Acc 0.2470 | Time(s) 72.4386\n",
            "Epoch 00022 | Train Loss 1.6745 | Train Acc 0.2676 | Val Loss 1.6154 | Val Acc 0.2494 | Time(s) 72.2111\n",
            "Epoch 00023 | Train Loss 1.7051 | Train Acc 0.2648 | Val Loss 1.6175 | Val Acc 0.2548 | Time(s) 72.2184\n",
            "Epoch 00024 | Train Loss 1.7085 | Train Acc 0.2825 | Val Loss 1.6142 | Val Acc 0.2308 | Time(s) 72.1110\n",
            "Epoch 00025 | Train Loss 1.6609 | Train Acc 0.2708 | Val Loss 1.6093 | Val Acc 0.2284 | Time(s) 72.0470\n",
            "Epoch 00026 | Train Loss 1.6734 | Train Acc 0.2764 | Val Loss 1.6021 | Val Acc 0.2248 | Time(s) 71.9725\n",
            "Epoch 00027 | Train Loss 1.6399 | Train Acc 0.2608 | Val Loss 1.5943 | Val Acc 0.2356 | Time(s) 71.9828\n",
            "Epoch 00028 | Train Loss 1.6180 | Train Acc 0.2728 | Val Loss 1.5896 | Val Acc 0.2548 | Time(s) 72.0221\n",
            "Epoch 00029 | Train Loss 1.6409 | Train Acc 0.2624 | Val Loss 1.5893 | Val Acc 0.2614 | Time(s) 71.9626\n",
            "Epoch 00030 | Train Loss 1.5967 | Train Acc 0.2732 | Val Loss 1.5903 | Val Acc 0.2764 | Time(s) 71.9245\n",
            "Epoch 00031 | Train Loss 1.5909 | Train Acc 0.2889 | Val Loss 1.5901 | Val Acc 0.2752 | Time(s) 71.8065\n",
            "Epoch 00032 | Train Loss 1.6015 | Train Acc 0.2772 | Val Loss 1.5872 | Val Acc 0.2752 | Time(s) 71.7535\n",
            "Epoch 00033 | Train Loss 1.6039 | Train Acc 0.2865 | Val Loss 1.5825 | Val Acc 0.2788 | Time(s) 71.6049\n",
            "Epoch 00034 | Train Loss 1.5940 | Train Acc 0.2933 | Val Loss 1.5777 | Val Acc 0.2782 | Time(s) 71.3200\n",
            "Epoch 00035 | Train Loss 1.5977 | Train Acc 0.2849 | Val Loss 1.5738 | Val Acc 0.2764 | Time(s) 71.0729\n",
            "Epoch 00036 | Train Loss 1.5520 | Train Acc 0.2969 | Val Loss 1.5722 | Val Acc 0.2903 | Time(s) 70.7725\n",
            "Epoch 00037 | Train Loss 1.5499 | Train Acc 0.3057 | Val Loss 1.5737 | Val Acc 0.2873 | Time(s) 70.5146\n",
            "Epoch 00038 | Train Loss 1.5629 | Train Acc 0.3045 | Val Loss 1.5768 | Val Acc 0.2686 | Time(s) 70.2927\n",
            "Epoch 00039 | Train Loss 1.5624 | Train Acc 0.2997 | Val Loss 1.5781 | Val Acc 0.2668 | Time(s) 70.0537\n",
            "Epoch 00040 | Train Loss 1.5524 | Train Acc 0.3205 | Val Loss 1.5762 | Val Acc 0.2716 | Time(s) 69.8447\n",
            "Epoch 00041 | Train Loss 1.5541 | Train Acc 0.3049 | Val Loss 1.5733 | Val Acc 0.2903 | Time(s) 69.6756\n",
            "Epoch 00042 | Train Loss 1.5411 | Train Acc 0.3081 | Val Loss 1.5712 | Val Acc 0.2993 | Time(s) 69.5493\n",
            "Epoch 00043 | Train Loss 1.5570 | Train Acc 0.3057 | Val Loss 1.5701 | Val Acc 0.3035 | Time(s) 69.3580\n",
            "Epoch 00044 | Train Loss 1.5323 | Train Acc 0.3161 | Val Loss 1.5704 | Val Acc 0.2975 | Time(s) 69.2074\n",
            "Epoch 00045 | Train Loss 1.5479 | Train Acc 0.3173 | Val Loss 1.5717 | Val Acc 0.2921 | Time(s) 69.0675\n",
            "Epoch 00046 | Train Loss 1.5348 | Train Acc 0.3185 | Val Loss 1.5724 | Val Acc 0.2861 | Time(s) 68.9003\n",
            "Epoch 00047 | Train Loss 1.5296 | Train Acc 0.3101 | Val Loss 1.5723 | Val Acc 0.2837 | Time(s) 68.7369\n",
            "Epoch 00048 | Train Loss 1.5413 | Train Acc 0.3157 | Val Loss 1.5712 | Val Acc 0.2788 | Time(s) 68.9344\n",
            "Epoch 00049 | Train Loss 1.5270 | Train Acc 0.3201 | Val Loss 1.5694 | Val Acc 0.2921 | Time(s) 69.0895\n",
            "Epoch 00050 | Train Loss 1.5364 | Train Acc 0.3017 | Val Loss 1.5681 | Val Acc 0.2987 | Time(s) 69.1576\n",
            "Epoch 00051 | Train Loss 1.5107 | Train Acc 0.3097 | Val Loss 1.5676 | Val Acc 0.2951 | Time(s) 69.2134\n",
            "Epoch 00052 | Train Loss 1.5311 | Train Acc 0.3225 | Val Loss 1.5684 | Val Acc 0.3005 | Time(s) 69.2228\n",
            "Epoch 00053 | Train Loss 1.4980 | Train Acc 0.3281 | Val Loss 1.5693 | Val Acc 0.2987 | Time(s) 69.2462\n",
            "Epoch 00054 | Train Loss 1.5363 | Train Acc 0.3125 | Val Loss 1.5705 | Val Acc 0.3023 | Time(s) 69.3850\n",
            "Epoch 00055 | Train Loss 1.4936 | Train Acc 0.3289 | Val Loss 1.5701 | Val Acc 0.2987 | Time(s) 69.4562\n",
            "Epoch 00056 | Train Loss 1.5105 | Train Acc 0.3169 | Val Loss 1.5695 | Val Acc 0.3011 | Time(s) 69.5038\n",
            "Epoch 00057 | Train Loss 1.5100 | Train Acc 0.3249 | Val Loss 1.5696 | Val Acc 0.2987 | Time(s) 69.5665\n",
            "Epoch 00058 | Train Loss 1.5187 | Train Acc 0.3161 | Val Loss 1.5703 | Val Acc 0.2987 | Time(s) 69.5782\n",
            "Epoch 00059 | Train Loss 1.5014 | Train Acc 0.3333 | Val Loss 1.5704 | Val Acc 0.2981 | Time(s) 69.6489\n",
            "Epoch 00060 | Train Loss 1.5123 | Train Acc 0.3101 | Val Loss 1.5699 | Val Acc 0.2987 | Time(s) 69.6906\n",
            "Epoch 00061 | Train Loss 1.5115 | Train Acc 0.3153 | Val Loss 1.5689 | Val Acc 0.3017 | Time(s) 69.7405\n",
            "Epoch 00062 | Train Loss 1.4922 | Train Acc 0.3337 | Val Loss 1.5682 | Val Acc 0.3005 | Time(s) 69.7396\n",
            "Epoch 00063 | Train Loss 1.4845 | Train Acc 0.3345 | Val Loss 1.5679 | Val Acc 0.3095 | Time(s) 69.7829\n",
            "Epoch 00064 | Train Loss 1.4840 | Train Acc 0.3353 | Val Loss 1.5677 | Val Acc 0.3059 | Time(s) 69.8192\n",
            "Epoch 00065 | Train Loss 1.5180 | Train Acc 0.3333 | Val Loss 1.5689 | Val Acc 0.3083 | Time(s) 69.8131\n",
            "Epoch 00066 | Train Loss 1.5191 | Train Acc 0.3257 | Val Loss 1.5711 | Val Acc 0.3005 | Time(s) 69.8052\n",
            "Epoch 00067 | Train Loss 1.4965 | Train Acc 0.3225 | Val Loss 1.5727 | Val Acc 0.2999 | Time(s) 69.8383\n",
            "Epoch 00068 | Train Loss 1.4870 | Train Acc 0.3337 | Val Loss 1.5725 | Val Acc 0.3017 | Time(s) 69.8784\n",
            "Epoch 00069 | Train Loss 1.5034 | Train Acc 0.3169 | Val Loss 1.5707 | Val Acc 0.3077 | Time(s) 69.8641\n",
            "Epoch 00070 | Train Loss 1.5097 | Train Acc 0.3129 | Val Loss 1.5672 | Val Acc 0.3047 | Time(s) 69.8537\n",
            "Epoch 00071 | Train Loss 1.4761 | Train Acc 0.3345 | Val Loss 1.5652 | Val Acc 0.2987 | Time(s) 69.9529\n",
            "Epoch 00072 | Train Loss 1.4904 | Train Acc 0.3081 | Val Loss 1.5634 | Val Acc 0.2981 | Time(s) 70.0016\n",
            "Epoch 00073 | Train Loss 1.5055 | Train Acc 0.3213 | Val Loss 1.5621 | Val Acc 0.3035 | Time(s) 70.1329\n",
            "Epoch 00074 | Train Loss 1.4907 | Train Acc 0.3269 | Val Loss 1.5619 | Val Acc 0.3089 | Time(s) 70.2656\n",
            "Epoch 00075 | Train Loss 1.4854 | Train Acc 0.3385 | Val Loss 1.5631 | Val Acc 0.3101 | Time(s) 70.3567\n",
            "Epoch 00076 | Train Loss 1.4909 | Train Acc 0.3201 | Val Loss 1.5666 | Val Acc 0.3005 | Time(s) 70.4393\n",
            "Epoch 00077 | Train Loss 1.4878 | Train Acc 0.3173 | Val Loss 1.5702 | Val Acc 0.3017 | Time(s) 70.5234\n",
            "Epoch 00078 | Train Loss 1.4885 | Train Acc 0.3213 | Val Loss 1.5713 | Val Acc 0.3035 | Time(s) 70.6020\n",
            "Epoch 00079 | Train Loss 1.5202 | Train Acc 0.3069 | Val Loss 1.5701 | Val Acc 0.3089 | Time(s) 70.6753\n",
            "Epoch 00080 | Train Loss 1.4946 | Train Acc 0.3425 | Val Loss 1.5681 | Val Acc 0.3029 | Time(s) 70.7851\n",
            "Epoch 00081 | Train Loss 1.4753 | Train Acc 0.3329 | Val Loss 1.5666 | Val Acc 0.3065 | Time(s) 70.8676\n",
            "Epoch 00082 | Train Loss 1.4984 | Train Acc 0.3205 | Val Loss 1.5665 | Val Acc 0.3083 | Time(s) 70.9299\n",
            "Epoch 00083 | Train Loss 1.5086 | Train Acc 0.3129 | Val Loss 1.5668 | Val Acc 0.3065 | Time(s) 70.9980\n",
            "Epoch 00084 | Train Loss 1.4861 | Train Acc 0.3237 | Val Loss 1.5666 | Val Acc 0.3065 | Time(s) 71.1028\n",
            "Epoch 00085 | Train Loss 1.4907 | Train Acc 0.3237 | Val Loss 1.5676 | Val Acc 0.3071 | Time(s) 71.1972\n",
            "Epoch 00086 | Train Loss 1.4810 | Train Acc 0.3313 | Val Loss 1.5697 | Val Acc 0.3083 | Time(s) 71.2910\n",
            "Epoch 00087 | Train Loss 1.5212 | Train Acc 0.3125 | Val Loss 1.5722 | Val Acc 0.3083 | Time(s) 71.3435\n",
            "Epoch 00088 | Train Loss 1.4768 | Train Acc 0.3405 | Val Loss 1.5746 | Val Acc 0.3083 | Time(s) 71.4019\n",
            "Epoch 00089 | Train Loss 1.5042 | Train Acc 0.3217 | Val Loss 1.5746 | Val Acc 0.3053 | Time(s) 71.4860\n",
            "Epoch 00090 | Train Loss 1.5076 | Train Acc 0.3297 | Val Loss 1.5726 | Val Acc 0.3059 | Time(s) 71.5708\n",
            "Epoch 00091 | Train Loss 1.4793 | Train Acc 0.3397 | Val Loss 1.5699 | Val Acc 0.3005 | Time(s) 71.6182\n",
            "Epoch 00092 | Train Loss 1.4637 | Train Acc 0.3385 | Val Loss 1.5683 | Val Acc 0.3041 | Time(s) 71.6962\n",
            "Epoch 00093 | Train Loss 1.4902 | Train Acc 0.3333 | Val Loss 1.5673 | Val Acc 0.3005 | Time(s) 71.7790\n",
            "Epoch 00094 | Train Loss 1.5055 | Train Acc 0.3373 | Val Loss 1.5674 | Val Acc 0.3029 | Time(s) 71.8555\n",
            "Epoch 00095 | Train Loss 1.4951 | Train Acc 0.3458 | Val Loss 1.5678 | Val Acc 0.3029 | Time(s) 71.8983\n",
            "Epoch 00096 | Train Loss 1.4862 | Train Acc 0.3429 | Val Loss 1.5687 | Val Acc 0.3083 | Time(s) 71.9537\n",
            "Epoch 00097 | Train Loss 1.4848 | Train Acc 0.3313 | Val Loss 1.5688 | Val Acc 0.3071 | Time(s) 71.9974\n",
            "Epoch 00098 | Train Loss 1.4776 | Train Acc 0.3329 | Val Loss 1.5694 | Val Acc 0.3023 | Time(s) 72.0290\n",
            "Epoch 00099 | Train Loss 1.4741 | Train Acc 0.3393 | Val Loss 1.5693 | Val Acc 0.3083 | Time(s) 72.0702\n",
            "Epoch 00100 | Train Loss 1.4808 | Train Acc 0.3289 | Val Loss 1.5688 | Val Acc 0.3065 | Time(s) 72.1095\n",
            "Epoch 00101 | Train Loss 1.4847 | Train Acc 0.3490 | Val Loss 1.5682 | Val Acc 0.3053 | Time(s) 72.1521\n",
            "Epoch 00102 | Train Loss 1.4622 | Train Acc 0.3486 | Val Loss 1.5676 | Val Acc 0.3023 | Time(s) 72.1853\n",
            "Epoch 00103 | Train Loss 1.4631 | Train Acc 0.3289 | Val Loss 1.5674 | Val Acc 0.3053 | Time(s) 72.2231\n",
            "Epoch 00104 | Train Loss 1.4778 | Train Acc 0.3498 | Val Loss 1.5674 | Val Acc 0.3077 | Time(s) 72.2593\n",
            "Epoch 00105 | Train Loss 1.4980 | Train Acc 0.3514 | Val Loss 1.5688 | Val Acc 0.3071 | Time(s) 72.3283\n",
            "Epoch 00106 | Train Loss 1.4737 | Train Acc 0.3429 | Val Loss 1.5699 | Val Acc 0.3089 | Time(s) 72.3936\n",
            "Epoch 00107 | Train Loss 1.4976 | Train Acc 0.3405 | Val Loss 1.5710 | Val Acc 0.3053 | Time(s) 72.3380\n",
            "Epoch 00108 | Train Loss 1.5061 | Train Acc 0.3369 | Val Loss 1.5705 | Val Acc 0.3041 | Time(s) 72.2740\n",
            "Epoch 00109 | Train Loss 1.4922 | Train Acc 0.3486 | Val Loss 1.5704 | Val Acc 0.3059 | Time(s) 72.1622\n",
            "Epoch 00110 | Train Loss 1.4847 | Train Acc 0.3401 | Val Loss 1.5707 | Val Acc 0.3059 | Time(s) 72.0694\n",
            "Epoch 00111 | Train Loss 1.4892 | Train Acc 0.3389 | Val Loss 1.5717 | Val Acc 0.3089 | Time(s) 72.1197\n",
            "Epoch 00112 | Train Loss 1.4805 | Train Acc 0.3438 | Val Loss 1.5732 | Val Acc 0.3077 | Time(s) 72.1624\n",
            "Epoch 00113 | Train Loss 1.4906 | Train Acc 0.3401 | Val Loss 1.5749 | Val Acc 0.3065 | Time(s) 72.1113\n",
            "Epoch 00114 | Train Loss 1.4863 | Train Acc 0.3373 | Val Loss 1.5750 | Val Acc 0.3083 | Time(s) 72.0926\n",
            "Epoch 00115 | Train Loss 1.5000 | Train Acc 0.3442 | Val Loss 1.5754 | Val Acc 0.3101 | Time(s) 72.0751\n",
            "Epoch 00116 | Train Loss 1.4607 | Train Acc 0.3526 | Val Loss 1.5771 | Val Acc 0.3083 | Time(s) 72.0600\n",
            "Epoch 00117 | Train Loss 1.5072 | Train Acc 0.3490 | Val Loss 1.5775 | Val Acc 0.3107 | Time(s) 71.9532\n",
            "Epoch 00118 | Train Loss 1.4964 | Train Acc 0.3373 | Val Loss 1.5749 | Val Acc 0.3059 | Time(s) 71.8987\n",
            "Epoch 00119 | Train Loss 1.4875 | Train Acc 0.3377 | Val Loss 1.5728 | Val Acc 0.3059 | Time(s) 71.8845\n",
            "Epoch 00120 | Train Loss 1.5132 | Train Acc 0.3285 | Val Loss 1.5716 | Val Acc 0.3107 | Time(s) 71.8566\n",
            "Epoch 00121 | Train Loss 1.4969 | Train Acc 0.3413 | Val Loss 1.5721 | Val Acc 0.3083 | Time(s) 71.8437\n",
            "Epoch 00122 | Train Loss 1.4755 | Train Acc 0.3450 | Val Loss 1.5738 | Val Acc 0.3119 | Time(s) 71.8326\n",
            "Epoch 00123 | Train Loss 1.5096 | Train Acc 0.3285 | Val Loss 1.5765 | Val Acc 0.3083 | Time(s) 71.8311\n",
            "Epoch 00124 | Train Loss 1.5032 | Train Acc 0.3289 | Val Loss 1.5772 | Val Acc 0.3089 | Time(s) 71.8465\n",
            "Epoch 00125 | Train Loss 1.5273 | Train Acc 0.3277 | Val Loss 1.5756 | Val Acc 0.3059 | Time(s) 71.8484\n",
            "Epoch 00126 | Train Loss 1.4816 | Train Acc 0.3277 | Val Loss 1.5728 | Val Acc 0.3095 | Time(s) 71.8272\n",
            "Epoch 00127 | Train Loss 1.5043 | Train Acc 0.3365 | Val Loss 1.5705 | Val Acc 0.3143 | Time(s) 71.8503\n",
            "Epoch 00128 | Train Loss 1.5129 | Train Acc 0.3293 | Val Loss 1.5702 | Val Acc 0.3113 | Time(s) 71.8834\n",
            "Epoch 00129 | Train Loss 1.4837 | Train Acc 0.3301 | Val Loss 1.5716 | Val Acc 0.3101 | Time(s) 71.9150\n",
            "Epoch 00130 | Train Loss 1.4786 | Train Acc 0.3450 | Val Loss 1.5726 | Val Acc 0.3083 | Time(s) 71.9027\n",
            "Epoch 00131 | Train Loss 1.5264 | Train Acc 0.3201 | Val Loss 1.5731 | Val Acc 0.3077 | Time(s) 71.8979\n",
            "Epoch 00132 | Train Loss 1.5183 | Train Acc 0.3221 | Val Loss 1.5736 | Val Acc 0.3077 | Time(s) 71.9415\n",
            "Epoch 00133 | Train Loss 1.4948 | Train Acc 0.3293 | Val Loss 1.5744 | Val Acc 0.3089 | Time(s) 71.9776\n",
            "Epoch 00134 | Train Loss 1.5066 | Train Acc 0.3409 | Val Loss 1.5749 | Val Acc 0.3083 | Time(s) 71.9996\n",
            "Epoch 00135 | Train Loss 1.5084 | Train Acc 0.3189 | Val Loss 1.5743 | Val Acc 0.3023 | Time(s) 71.9776\n",
            "Epoch 00136 | Train Loss 1.4953 | Train Acc 0.3349 | Val Loss 1.5731 | Val Acc 0.3041 | Time(s) 71.9563\n",
            "Epoch 00137 | Train Loss 1.5141 | Train Acc 0.3261 | Val Loss 1.5713 | Val Acc 0.3005 | Time(s) 71.9481\n",
            "Epoch 00138 | Train Loss 1.4837 | Train Acc 0.3446 | Val Loss 1.5715 | Val Acc 0.3029 | Time(s) 71.9278\n",
            "Epoch 00139 | Train Loss 1.5095 | Train Acc 0.3265 | Val Loss 1.5734 | Val Acc 0.3011 | Time(s) 71.9166\n",
            "Epoch 00140 | Train Loss 1.4893 | Train Acc 0.3277 | Val Loss 1.5769 | Val Acc 0.3011 | Time(s) 71.9421\n",
            "Epoch 00141 | Train Loss 1.5078 | Train Acc 0.3385 | Val Loss 1.5801 | Val Acc 0.2987 | Time(s) 71.9696\n",
            "Epoch 00142 | Train Loss 1.4847 | Train Acc 0.3313 | Val Loss 1.5818 | Val Acc 0.2987 | Time(s) 71.9748\n",
            "Epoch 00143 | Train Loss 1.4860 | Train Acc 0.3438 | Val Loss 1.5793 | Val Acc 0.2999 | Time(s) 71.9832\n",
            "Epoch 00144 | Train Loss 1.4802 | Train Acc 0.3381 | Val Loss 1.5773 | Val Acc 0.3011 | Time(s) 72.0104\n",
            "Epoch 00145 | Train Loss 1.4784 | Train Acc 0.3385 | Val Loss 1.5760 | Val Acc 0.3035 | Time(s) 72.0358\n",
            "Epoch 00146 | Train Loss 1.4719 | Train Acc 0.3317 | Val Loss 1.5751 | Val Acc 0.3041 | Time(s) 72.0131\n",
            "Epoch 00147 | Train Loss 1.4900 | Train Acc 0.3442 | Val Loss 1.5753 | Val Acc 0.3053 | Time(s) 72.0063\n",
            "Epoch 00148 | Train Loss 1.4833 | Train Acc 0.3450 | Val Loss 1.5754 | Val Acc 0.3041 | Time(s) 72.0019\n",
            "Epoch 00149 | Train Loss 1.5211 | Train Acc 0.3486 | Val Loss 1.5767 | Val Acc 0.3035 | Time(s) 71.9805\n",
            "Epoch 00150 | Train Loss 1.4659 | Train Acc 0.3506 | Val Loss 1.5790 | Val Acc 0.3011 | Time(s) 71.9967\n",
            "Epoch 00151 | Train Loss 1.4693 | Train Acc 0.3373 | Val Loss 1.5817 | Val Acc 0.2987 | Time(s) 71.9937\n",
            "Epoch 00152 | Train Loss 1.4707 | Train Acc 0.3381 | Val Loss 1.5822 | Val Acc 0.3017 | Time(s) 71.9805\n",
            "Epoch 00153 | Train Loss 1.4746 | Train Acc 0.3498 | Val Loss 1.5833 | Val Acc 0.3041 | Time(s) 72.0079\n",
            "Epoch 00154 | Train Loss 1.4955 | Train Acc 0.3454 | Val Loss 1.5836 | Val Acc 0.3053 | Time(s) 72.0528\n",
            "Epoch 00155 | Train Loss 1.4855 | Train Acc 0.3506 | Val Loss 1.5822 | Val Acc 0.3059 | Time(s) 72.0566\n",
            "Epoch 00156 | Train Loss 1.4831 | Train Acc 0.3429 | Val Loss 1.5812 | Val Acc 0.3095 | Time(s) 72.0318\n",
            "Epoch 00157 | Train Loss 1.5007 | Train Acc 0.3381 | Val Loss 1.5810 | Val Acc 0.3101 | Time(s) 72.0278\n",
            "Epoch 00158 | Train Loss 1.4680 | Train Acc 0.3674 | Val Loss 1.5801 | Val Acc 0.3065 | Time(s) 72.0533\n",
            "Epoch 00159 | Train Loss 1.4860 | Train Acc 0.3438 | Val Loss 1.5795 | Val Acc 0.3113 | Time(s) 72.0801\n",
            "Epoch 00160 | Train Loss 1.4865 | Train Acc 0.3514 | Val Loss 1.5807 | Val Acc 0.3125 | Time(s) 72.0436\n",
            "Epoch 00161 | Train Loss 1.4911 | Train Acc 0.3502 | Val Loss 1.5829 | Val Acc 0.3113 | Time(s) 72.0490\n",
            "Epoch 00162 | Train Loss 1.4911 | Train Acc 0.3458 | Val Loss 1.5839 | Val Acc 0.3095 | Time(s) 72.0284\n",
            "Epoch 00163 | Train Loss 1.4985 | Train Acc 0.3365 | Val Loss 1.5841 | Val Acc 0.3131 | Time(s) 72.0118\n",
            "Epoch 00164 | Train Loss 1.4792 | Train Acc 0.3462 | Val Loss 1.5839 | Val Acc 0.3107 | Time(s) 72.0078\n",
            "Epoch 00165 | Train Loss 1.4926 | Train Acc 0.3438 | Val Loss 1.5829 | Val Acc 0.3095 | Time(s) 72.0066\n",
            "Epoch 00166 | Train Loss 1.5215 | Train Acc 0.3442 | Val Loss 1.5815 | Val Acc 0.3077 | Time(s) 72.0460\n",
            "Epoch 00167 | Train Loss 1.4797 | Train Acc 0.3474 | Val Loss 1.5821 | Val Acc 0.3047 | Time(s) 72.0786\n",
            "Epoch 00168 | Train Loss 1.4482 | Train Acc 0.3466 | Val Loss 1.5839 | Val Acc 0.2969 | Time(s) 72.0969\n",
            "Epoch 00169 | Train Loss 1.4977 | Train Acc 0.3454 | Val Loss 1.5865 | Val Acc 0.2987 | Time(s) 72.1384\n",
            "Epoch 00170 | Train Loss 1.4805 | Train Acc 0.3442 | Val Loss 1.5849 | Val Acc 0.2939 | Time(s) 72.1677\n",
            "Epoch 00171 | Train Loss 1.5120 | Train Acc 0.3429 | Val Loss 1.5811 | Val Acc 0.2975 | Time(s) 72.1745\n",
            "Epoch 00172 | Train Loss 1.4860 | Train Acc 0.3361 | Val Loss 1.5798 | Val Acc 0.2963 | Time(s) 72.1058\n",
            "Epoch 00173 | Train Loss 1.4875 | Train Acc 0.3442 | Val Loss 1.5816 | Val Acc 0.2999 | Time(s) 72.0943\n",
            "Epoch 00174 | Train Loss 1.4521 | Train Acc 0.3429 | Val Loss 1.5852 | Val Acc 0.3035 | Time(s) 72.0550\n",
            "Epoch 00175 | Train Loss 1.5075 | Train Acc 0.3369 | Val Loss 1.5881 | Val Acc 0.3029 | Time(s) 72.0122\n",
            "Epoch 00176 | Train Loss 1.4832 | Train Acc 0.3462 | Val Loss 1.5911 | Val Acc 0.2951 | Time(s) 72.0077\n",
            "Epoch 00177 | Train Loss 1.5018 | Train Acc 0.3550 | Val Loss 1.5944 | Val Acc 0.2951 | Time(s) 71.9731\n",
            "Epoch 00178 | Train Loss 1.4762 | Train Acc 0.3466 | Val Loss 1.5947 | Val Acc 0.2915 | Time(s) 71.9503\n",
            "Epoch 00179 | Train Loss 1.4836 | Train Acc 0.3433 | Val Loss 1.5930 | Val Acc 0.2939 | Time(s) 71.9517\n",
            "Epoch 00180 | Train Loss 1.4754 | Train Acc 0.3490 | Val Loss 1.5916 | Val Acc 0.2927 | Time(s) 71.9179\n",
            "Epoch 00181 | Train Loss 1.4908 | Train Acc 0.3478 | Val Loss 1.5898 | Val Acc 0.2897 | Time(s) 71.8824\n",
            "Epoch 00182 | Train Loss 1.4868 | Train Acc 0.3510 | Val Loss 1.5884 | Val Acc 0.2963 | Time(s) 71.8901\n",
            "Epoch 00183 | Train Loss 1.4842 | Train Acc 0.3586 | Val Loss 1.5878 | Val Acc 0.2999 | Time(s) 71.9210\n",
            "Epoch 00184 | Train Loss 1.4807 | Train Acc 0.3349 | Val Loss 1.5876 | Val Acc 0.3035 | Time(s) 71.9516\n",
            "Epoch 00185 | Train Loss 1.4764 | Train Acc 0.3478 | Val Loss 1.5883 | Val Acc 0.2999 | Time(s) 71.9831\n",
            "Epoch 00186 | Train Loss 1.4888 | Train Acc 0.3385 | Val Loss 1.5900 | Val Acc 0.3005 | Time(s) 72.0133\n",
            "Epoch 00187 | Train Loss 1.4668 | Train Acc 0.3558 | Val Loss 1.5914 | Val Acc 0.3011 | Time(s) 72.0410\n",
            "Epoch 00188 | Train Loss 1.5131 | Train Acc 0.3413 | Val Loss 1.5917 | Val Acc 0.2999 | Time(s) 72.0022\n",
            "Epoch 00189 | Train Loss 1.4777 | Train Acc 0.3466 | Val Loss 1.5911 | Val Acc 0.2999 | Time(s) 72.0019\n",
            "Epoch 00190 | Train Loss 1.4690 | Train Acc 0.3550 | Val Loss 1.5906 | Val Acc 0.3047 | Time(s) 72.0060\n",
            "Epoch 00191 | Train Loss 1.4693 | Train Acc 0.3429 | Val Loss 1.5891 | Val Acc 0.3041 | Time(s) 72.0053\n",
            "Epoch 00192 | Train Loss 1.4694 | Train Acc 0.3474 | Val Loss 1.5865 | Val Acc 0.3077 | Time(s) 71.9819\n",
            "Epoch 00193 | Train Loss 1.4675 | Train Acc 0.3458 | Val Loss 1.5834 | Val Acc 0.3053 | Time(s) 71.9664\n",
            "Epoch 00194 | Train Loss 1.4840 | Train Acc 0.3470 | Val Loss 1.5809 | Val Acc 0.3047 | Time(s) 71.9499\n",
            "Epoch 00195 | Train Loss 1.4673 | Train Acc 0.3502 | Val Loss 1.5792 | Val Acc 0.3011 | Time(s) 71.9381\n",
            "Epoch 00196 | Train Loss 1.4835 | Train Acc 0.3466 | Val Loss 1.5792 | Val Acc 0.3047 | Time(s) 71.9342\n",
            "Epoch 00197 | Train Loss 1.4727 | Train Acc 0.3534 | Val Loss 1.5791 | Val Acc 0.3053 | Time(s) 71.9188\n",
            "Epoch 00198 | Train Loss 1.4707 | Train Acc 0.3506 | Val Loss 1.5810 | Val Acc 0.3011 | Time(s) 71.9308\n",
            "Epoch 00199 | Train Loss 1.4666 | Train Acc 0.3550 | Val Loss 1.5834 | Val Acc 0.3005 | Time(s) 71.9184\n",
            "Epoch 00200 | Train Loss 1.4875 | Train Acc 0.3582 | Val Loss 1.5849 | Val Acc 0.3011 | Time(s) 71.9122\n",
            "Epoch 00201 | Train Loss 1.4739 | Train Acc 0.3442 | Val Loss 1.5865 | Val Acc 0.2999 | Time(s) 71.8953\n",
            "Epoch 00202 | Train Loss 1.4632 | Train Acc 0.3526 | Val Loss 1.5872 | Val Acc 0.2927 | Time(s) 71.8955\n",
            "Epoch 00203 | Train Loss 1.4393 | Train Acc 0.3582 | Val Loss 1.5877 | Val Acc 0.2993 | Time(s) 71.8818\n",
            "Epoch 00204 | Train Loss 1.4528 | Train Acc 0.3526 | Val Loss 1.5868 | Val Acc 0.3035 | Time(s) 71.8938\n",
            "Epoch 00205 | Train Loss 1.4519 | Train Acc 0.3642 | Val Loss 1.5835 | Val Acc 0.2999 | Time(s) 71.8957\n",
            "Epoch 00206 | Train Loss 1.4938 | Train Acc 0.3470 | Val Loss 1.5819 | Val Acc 0.3005 | Time(s) 71.9220\n",
            "Epoch 00207 | Train Loss 1.4761 | Train Acc 0.3522 | Val Loss 1.5814 | Val Acc 0.2963 | Time(s) 71.9456\n",
            "Epoch 00208 | Train Loss 1.4634 | Train Acc 0.3450 | Val Loss 1.5825 | Val Acc 0.2981 | Time(s) 71.9636\n",
            "Epoch 00209 | Train Loss 1.4855 | Train Acc 0.3421 | Val Loss 1.5844 | Val Acc 0.2969 | Time(s) 71.9932\n",
            "Epoch 00210 | Train Loss 1.4589 | Train Acc 0.3734 | Val Loss 1.5864 | Val Acc 0.3011 | Time(s) 72.0279\n",
            "Epoch 00211 | Train Loss 1.4679 | Train Acc 0.3550 | Val Loss 1.5908 | Val Acc 0.3023 | Time(s) 72.0445\n",
            "Epoch 00212 | Train Loss 1.4570 | Train Acc 0.3530 | Val Loss 1.5966 | Val Acc 0.3017 | Time(s) 72.0807\n",
            "Epoch 00213 | Train Loss 1.4378 | Train Acc 0.3578 | Val Loss 1.5985 | Val Acc 0.3011 | Time(s) 72.1134\n",
            "Epoch 00214 | Train Loss 1.4841 | Train Acc 0.3498 | Val Loss 1.5958 | Val Acc 0.2987 | Time(s) 72.1154\n",
            "Epoch 00215 | Train Loss 1.4689 | Train Acc 0.3442 | Val Loss 1.5928 | Val Acc 0.2975 | Time(s) 72.1013\n",
            "Epoch 00216 | Train Loss 1.4341 | Train Acc 0.3474 | Val Loss 1.5899 | Val Acc 0.3059 | Time(s) 72.0838\n",
            "Epoch 00217 | Train Loss 1.4544 | Train Acc 0.3498 | Val Loss 1.5900 | Val Acc 0.3053 | Time(s) 72.1094\n",
            "Epoch 00218 | Train Loss 1.4446 | Train Acc 0.3566 | Val Loss 1.5915 | Val Acc 0.3041 | Time(s) 72.1371\n",
            "Epoch 00219 | Train Loss 1.4646 | Train Acc 0.3570 | Val Loss 1.5931 | Val Acc 0.2969 | Time(s) 72.1191\n",
            "Epoch 00220 | Train Loss 1.4503 | Train Acc 0.3650 | Val Loss 1.5954 | Val Acc 0.2951 | Time(s) 72.1197\n",
            "Epoch 00221 | Train Loss 1.4693 | Train Acc 0.3542 | Val Loss 1.5979 | Val Acc 0.2909 | Time(s) 72.0994\n",
            "Epoch 00222 | Train Loss 1.4284 | Train Acc 0.3746 | Val Loss 1.5998 | Val Acc 0.2957 | Time(s) 72.0700\n",
            "Epoch 00223 | Train Loss 1.4516 | Train Acc 0.3606 | Val Loss 1.6008 | Val Acc 0.2951 | Time(s) 72.0613\n",
            "Epoch 00224 | Train Loss 1.4691 | Train Acc 0.3570 | Val Loss 1.6004 | Val Acc 0.2975 | Time(s) 72.0415\n",
            "Epoch 00225 | Train Loss 1.4773 | Train Acc 0.3482 | Val Loss 1.5981 | Val Acc 0.2999 | Time(s) 72.0395\n",
            "Epoch 00226 | Train Loss 1.4795 | Train Acc 0.3417 | Val Loss 1.5977 | Val Acc 0.3011 | Time(s) 72.0352\n",
            "Epoch 00227 | Train Loss 1.4420 | Train Acc 0.3634 | Val Loss 1.5979 | Val Acc 0.3053 | Time(s) 72.0330\n",
            "\n",
            "Epoch 00228 | Train Loss 1.4644 | Train Acc 0.3454 | Val Loss 1.5982 | Val Acc 0.3089 | Time(s) 72.0307\n",
            "\n",
            "Epoch 00229 | Train Loss 1.4560 | Train Acc 0.3433 | Val Loss 1.5973 | Val Acc 0.3077 | Time(s) 72.0157\n",
            "\n",
            "Epoch 00230 | Train Loss 1.4491 | Train Acc 0.3466 | Val Loss 1.5954 | Val Acc 0.3059 | Time(s) 72.0245\n",
            "\n",
            "Epoch 00231 | Train Loss 1.4588 | Train Acc 0.3494 | Val Loss 1.5937 | Val Acc 0.3029 | Time(s) 72.0270\n",
            "\n",
            "Epoch 00232 | Train Loss 1.4757 | Train Acc 0.3586 | Val Loss 1.5924 | Val Acc 0.3011 | Time(s) 72.0578\n",
            "\n",
            "Epoch 00233 | Train Loss 1.4767 | Train Acc 0.3502 | Val Loss 1.5915 | Val Acc 0.3035 | Time(s) 72.0894\n",
            "\n",
            "Epoch 00234 | Train Loss 1.4302 | Train Acc 0.3562 | Val Loss 1.5899 | Val Acc 0.2993 | Time(s) 72.1092\n",
            "\n",
            "Epoch 00235 | Train Loss 1.4655 | Train Acc 0.3530 | Val Loss 1.5900 | Val Acc 0.2981 | Time(s) 72.1291\n",
            "\n",
            "Epoch 00236 | Train Loss 1.4773 | Train Acc 0.3590 | Val Loss 1.5906 | Val Acc 0.2999 | Time(s) 72.1440\n",
            "\n",
            "Epoch 00237 | Train Loss 1.4505 | Train Acc 0.3466 | Val Loss 1.5914 | Val Acc 0.2999 | Time(s) 72.1338\n",
            "\n",
            "Epoch 00238 | Train Loss 1.4689 | Train Acc 0.3514 | Val Loss 1.5929 | Val Acc 0.3017 | Time(s) 72.1253\n",
            "\n",
            "Epoch 00239 | Train Loss 1.4559 | Train Acc 0.3413 | Val Loss 1.5937 | Val Acc 0.3065 | Time(s) 72.1236\n",
            "\n",
            "Epoch 00240 | Train Loss 1.4498 | Train Acc 0.3594 | Val Loss 1.5927 | Val Acc 0.3047 | Time(s) 72.1133\n",
            "\n",
            "Epoch 00241 | Train Loss 1.4536 | Train Acc 0.3558 | Val Loss 1.5907 | Val Acc 0.3053 | Time(s) 72.1108\n",
            "\n",
            "Epoch 00242 | Train Loss 1.4551 | Train Acc 0.3438 | Val Loss 1.5882 | Val Acc 0.3053 | Time(s) 72.1087\n",
            "\n",
            "Epoch 00243 | Train Loss 1.4695 | Train Acc 0.3438 | Val Loss 1.5860 | Val Acc 0.2969 | Time(s) 72.0987\n",
            "\n",
            "Epoch 00244 | Train Loss 1.4475 | Train Acc 0.3638 | Val Loss 1.5862 | Val Acc 0.2963 | Time(s) 72.0889\n",
            "\n",
            "Epoch 00245 | Train Loss 1.4428 | Train Acc 0.3502 | Val Loss 1.5883 | Val Acc 0.2957 | Time(s) 72.0772\n",
            "\n",
            "Epoch 00246 | Train Loss 1.4293 | Train Acc 0.3610 | Val Loss 1.5894 | Val Acc 0.2939 | Time(s) 72.0819\n",
            "\n",
            "Epoch 00247 | Train Loss 1.4397 | Train Acc 0.3470 | Val Loss 1.5904 | Val Acc 0.2999 | Time(s) 72.0798\n",
            "\n",
            "Epoch 00248 | Train Loss 1.4430 | Train Acc 0.3594 | Val Loss 1.5904 | Val Acc 0.3035 | Time(s) 72.0765\n",
            "\n",
            "Epoch 00249 | Train Loss 1.4300 | Train Acc 0.3534 | Val Loss 1.5895 | Val Acc 0.3059 | Time(s) 72.0752\n",
            "\n",
            "Epoch 00250 | Train Loss 1.4602 | Train Acc 0.3466 | Val Loss 1.5887 | Val Acc 0.3077 | Time(s) 72.0774\n",
            "\n",
            "Epoch 00251 | Train Loss 1.4410 | Train Acc 0.3498 | Val Loss 1.5879 | Val Acc 0.3029 | Time(s) 72.0751\n",
            "\n",
            "Epoch 00252 | Train Loss 1.4501 | Train Acc 0.3530 | Val Loss 1.5860 | Val Acc 0.2987 | Time(s) 72.0629\n",
            "\n",
            "Epoch 00253 | Train Loss 1.4489 | Train Acc 0.3385 | Val Loss 1.5847 | Val Acc 0.2993 | Time(s) 72.0611\n",
            "\n",
            "Epoch 00254 | Train Loss 1.4535 | Train Acc 0.3450 | Val Loss 1.5844 | Val Acc 0.2987 | Time(s) 72.0538\n",
            "\n",
            "Epoch 00255 | Train Loss 1.4440 | Train Acc 0.3466 | Val Loss 1.5852 | Val Acc 0.2963 | Time(s) 72.0444\n",
            "\n",
            "Epoch 00256 | Train Loss 1.4369 | Train Acc 0.3498 | Val Loss 1.5860 | Val Acc 0.2987 | Time(s) 72.0329\n",
            "\n",
            "Epoch 00257 | Train Loss 1.4524 | Train Acc 0.3538 | Val Loss 1.5869 | Val Acc 0.3017 | Time(s) 72.0294\n",
            "\n",
            "Epoch 00258 | Train Loss 1.4333 | Train Acc 0.3442 | Val Loss 1.5881 | Val Acc 0.2999 | Time(s) 72.0254\n",
            "\n",
            "Epoch 00259 | Train Loss 1.4457 | Train Acc 0.3405 | Val Loss 1.5884 | Val Acc 0.2987 | Time(s) 72.0305\n",
            "\n",
            "Epoch 00260 | Train Loss 1.4670 | Train Acc 0.3482 | Val Loss 1.5885 | Val Acc 0.2969 | Time(s) 72.0258\n",
            "\n",
            "Epoch 00261 | Train Loss 1.4603 | Train Acc 0.3562 | Val Loss 1.5889 | Val Acc 0.2993 | Time(s) 72.0219\n",
            "\n",
            "Epoch 00262 | Train Loss 1.4192 | Train Acc 0.3618 | Val Loss 1.5891 | Val Acc 0.2993 | Time(s) 72.0172\n",
            "\n",
            "Epoch 00263 | Train Loss 1.4158 | Train Acc 0.3578 | Val Loss 1.5891 | Val Acc 0.2987 | Time(s) 72.0170\n",
            "\n",
            "Epoch 00264 | Train Loss 1.4417 | Train Acc 0.3578 | Val Loss 1.5896 | Val Acc 0.2933 | Time(s) 72.0042\n",
            "\n",
            "Epoch 00265 | Train Loss 1.4392 | Train Acc 0.3590 | Val Loss 1.5899 | Val Acc 0.2939 | Time(s) 72.0033\n",
            "\n",
            "Epoch 00266 | Train Loss 1.4529 | Train Acc 0.3502 | Val Loss 1.5902 | Val Acc 0.2945 | Time(s) 71.9923\n",
            "\n",
            "Epoch 00267 | Train Loss 1.4491 | Train Acc 0.3438 | Val Loss 1.5911 | Val Acc 0.2957 | Time(s) 71.9684\n",
            "\n",
            "Epoch 00268 | Train Loss 1.4425 | Train Acc 0.3578 | Val Loss 1.5915 | Val Acc 0.2939 | Time(s) 71.9421\n",
            "\n",
            "Epoch 00269 | Train Loss 1.4309 | Train Acc 0.3662 | Val Loss 1.5939 | Val Acc 0.2999 | Time(s) 71.9368\n",
            "\n",
            "Epoch 00270 | Train Loss 1.4339 | Train Acc 0.3626 | Val Loss 1.5951 | Val Acc 0.2999 | Time(s) 71.9358\n",
            "\n",
            "Epoch 00271 | Train Loss 1.4354 | Train Acc 0.3558 | Val Loss 1.5960 | Val Acc 0.2963 | Time(s) 71.9503\n",
            "\n",
            "Epoch 00272 | Train Loss 1.4399 | Train Acc 0.3686 | Val Loss 1.5977 | Val Acc 0.2945 | Time(s) 71.9686\n",
            "\n",
            "Epoch 00273 | Train Loss 1.4358 | Train Acc 0.3526 | Val Loss 1.5979 | Val Acc 0.2897 | Time(s) 71.9809\n",
            "\n",
            "Epoch 00274 | Train Loss 1.4322 | Train Acc 0.3690 | Val Loss 1.5982 | Val Acc 0.2903 | Time(s) 72.0045\n",
            "\n",
            "Epoch 00275 | Train Loss 1.4444 | Train Acc 0.3582 | Val Loss 1.5961 | Val Acc 0.2897 | Time(s) 72.0290\n",
            "\n",
            "Epoch 00276 | Train Loss 1.4367 | Train Acc 0.3654 | Val Loss 1.5923 | Val Acc 0.2933 | Time(s) 72.0174\n",
            "\n",
            "Epoch 00277 | Train Loss 1.4221 | Train Acc 0.3682 | Val Loss 1.5903 | Val Acc 0.2957 | Time(s) 72.0057\n",
            "\n",
            "Epoch 00278 | Train Loss 1.4454 | Train Acc 0.3578 | Val Loss 1.5883 | Val Acc 0.2963 | Time(s) 71.9969\n",
            "\n",
            "Epoch 00279 | Train Loss 1.4349 | Train Acc 0.3734 | Val Loss 1.5874 | Val Acc 0.2969 | Time(s) 71.9875\n",
            "\n",
            "Epoch 00280 | Train Loss 1.4223 | Train Acc 0.3622 | Val Loss 1.5871 | Val Acc 0.2987 | Time(s) 71.9864\n",
            "\n",
            "Epoch 00281 | Train Loss 1.4403 | Train Acc 0.3554 | Val Loss 1.5880 | Val Acc 0.2975 | Time(s) 71.9992\n",
            "\n",
            "Epoch 00282 | Train Loss 1.4308 | Train Acc 0.3710 | Val Loss 1.5900 | Val Acc 0.2969 | Time(s) 72.0224\n",
            "\n",
            "Epoch 00283 | Train Loss 1.4148 | Train Acc 0.3630 | Val Loss 1.5919 | Val Acc 0.2939 | Time(s) 72.0453\n",
            "\n",
            "Epoch 00284 | Train Loss 1.4365 | Train Acc 0.3562 | Val Loss 1.5929 | Val Acc 0.2921 | Time(s) 72.0589\n",
            "\n",
            "Epoch 00285 | Train Loss 1.4403 | Train Acc 0.3658 | Val Loss 1.5925 | Val Acc 0.2939 | Time(s) 72.0793\n",
            "\n",
            "Epoch 00286 | Train Loss 1.4454 | Train Acc 0.3638 | Val Loss 1.5912 | Val Acc 0.2951 | Time(s) 72.0938\n",
            "\n",
            "Epoch 00287 | Train Loss 1.4292 | Train Acc 0.3758 | Val Loss 1.5905 | Val Acc 0.2963 | Time(s) 72.1161\n",
            "\n",
            "Epoch 00288 | Train Loss 1.4249 | Train Acc 0.3594 | Val Loss 1.5908 | Val Acc 0.2945 | Time(s) 72.1398\n",
            "\n",
            "Epoch 00289 | Train Loss 1.4190 | Train Acc 0.3750 | Val Loss 1.5908 | Val Acc 0.2957 | Time(s) 72.1488\n",
            "\n",
            "Epoch 00290 | Train Loss 1.4535 | Train Acc 0.3546 | Val Loss 1.5919 | Val Acc 0.2987 | Time(s) 72.1723\n",
            "\n",
            "Epoch 00291 | Train Loss 1.4486 | Train Acc 0.3502 | Val Loss 1.5924 | Val Acc 0.2945 | Time(s) 72.1804\n",
            "\n",
            "Epoch 00292 | Train Loss 1.4383 | Train Acc 0.3602 | Val Loss 1.5934 | Val Acc 0.2891 | Time(s) 72.1834\n",
            "\n",
            "Epoch 00293 | Train Loss 1.4395 | Train Acc 0.3598 | Val Loss 1.5930 | Val Acc 0.2885 | Time(s) 72.1742\n",
            "\n",
            "Epoch 00294 | Train Loss 1.4254 | Train Acc 0.3614 | Val Loss 1.5921 | Val Acc 0.2921 | Time(s) 72.1699\n",
            "\n",
            "Epoch 00295 | Train Loss 1.4238 | Train Acc 0.3638 | Val Loss 1.5910 | Val Acc 0.2963 | Time(s) 72.1599\n",
            "\n",
            "Epoch 00296 | Train Loss 1.4017 | Train Acc 0.3798 | Val Loss 1.5913 | Val Acc 0.3017 | Time(s) 72.1581\n",
            "\n",
            "Epoch 00297 | Train Loss 1.4524 | Train Acc 0.3514 | Val Loss 1.5914 | Val Acc 0.3035 | Time(s) 72.1549\n",
            "\n",
            "Epoch 00298 | Train Loss 1.4204 | Train Acc 0.3670 | Val Loss 1.5915 | Val Acc 0.3023 | Time(s) 72.1458\n",
            "\n",
            "Epoch 00299 | Train Loss 1.4233 | Train Acc 0.3642 | Val Loss 1.5919 | Val Acc 0.3053 | Time(s) 72.1257\n",
            "\n",
            "Epoch 00300 | Train Loss 1.4446 | Train Acc 0.3614 | Val Loss 1.5926 | Val Acc 0.3053 | Time(s) 72.0813\n",
            "\n",
            "Epoch 00301 | Train Loss 1.4143 | Train Acc 0.3694 | Val Loss 1.5937 | Val Acc 0.3035 | Time(s) 72.0443\n",
            "\n",
            "Epoch 00302 | Train Loss 1.4232 | Train Acc 0.3578 | Val Loss 1.5940 | Val Acc 0.3041 | Time(s) 72.0010\n",
            "\n",
            "Epoch 00303 | Train Loss 1.4374 | Train Acc 0.3690 | Val Loss 1.5941 | Val Acc 0.2975 | Time(s) 71.9639\n",
            "\n",
            "Epoch 00304 | Train Loss 1.4105 | Train Acc 0.3598 | Val Loss 1.5930 | Val Acc 0.2963 | Time(s) 71.9217\n",
            "\n",
            "Epoch 00305 | Train Loss 1.4352 | Train Acc 0.3606 | Val Loss 1.5906 | Val Acc 0.3005 | Time(s) 71.8844\n",
            "\n",
            "Epoch 00306 | Train Loss 1.4358 | Train Acc 0.3682 | Val Loss 1.5892 | Val Acc 0.2969 | Time(s) 71.8475\n",
            "\n",
            "Epoch 00307 | Train Loss 1.4261 | Train Acc 0.3582 | Val Loss 1.5901 | Val Acc 0.2999 | Time(s) 71.8136\n",
            "\n",
            "Epoch 00308 | Train Loss 1.4408 | Train Acc 0.3606 | Val Loss 1.5916 | Val Acc 0.3029 | Time(s) 71.8036\n",
            "\n",
            "Epoch 00309 | Train Loss 1.4267 | Train Acc 0.3586 | Val Loss 1.5939 | Val Acc 0.3011 | Time(s) 71.7941\n",
            "\n",
            "Epoch 00310 | Train Loss 1.4131 | Train Acc 0.3650 | Val Loss 1.5965 | Val Acc 0.2957 | Time(s) 71.7854\n",
            "\n",
            "Epoch 00311 | Train Loss 1.4178 | Train Acc 0.3622 | Val Loss 1.5976 | Val Acc 0.2969 | Time(s) 71.7741\n",
            "\n",
            "Epoch 00312 | Train Loss 1.4109 | Train Acc 0.3610 | Val Loss 1.5964 | Val Acc 0.3023 | Time(s) 71.7666\n",
            "\n",
            "Epoch 00313 | Train Loss 1.4286 | Train Acc 0.3642 | Val Loss 1.5949 | Val Acc 0.3011 | Time(s) 71.7579\n",
            "\n",
            "Epoch 00314 | Train Loss 1.4191 | Train Acc 0.3694 | Val Loss 1.5930 | Val Acc 0.3017 | Time(s) 71.7546\n",
            "\n",
            "Epoch 00315 | Train Loss 1.4202 | Train Acc 0.3666 | Val Loss 1.5910 | Val Acc 0.3011 | Time(s) 71.7456\n",
            "\n",
            "Epoch 00316 | Train Loss 1.4348 | Train Acc 0.3590 | Val Loss 1.5896 | Val Acc 0.2999 | Time(s) 71.7431\n",
            "\n",
            "Epoch 00317 | Train Loss 1.4240 | Train Acc 0.3642 | Val Loss 1.5886 | Val Acc 0.3011 | Time(s) 71.7424\n",
            "\n",
            "Epoch 00318 | Train Loss 1.4183 | Train Acc 0.3658 | Val Loss 1.5884 | Val Acc 0.2993 | Time(s) 71.7605\n",
            "\n",
            "Epoch 00319 | Train Loss 1.4383 | Train Acc 0.3662 | Val Loss 1.5886 | Val Acc 0.3017 | Time(s) 71.7808\n",
            "\n",
            "Epoch 00320 | Train Loss 1.4388 | Train Acc 0.3602 | Val Loss 1.5890 | Val Acc 0.3065 | Time(s) 71.7852\n",
            "\n",
            "Epoch 00321 | Train Loss 1.4335 | Train Acc 0.3622 | Val Loss 1.5902 | Val Acc 0.3023 | Time(s) 71.7961\n",
            "\n",
            "Epoch 00322 | Train Loss 1.4118 | Train Acc 0.3634 | Val Loss 1.5913 | Val Acc 0.2999 | Time(s) 71.8179\n",
            "\n",
            "Epoch 00323 | Train Loss 1.4178 | Train Acc 0.3574 | Val Loss 1.5915 | Val Acc 0.3017 | Time(s) 71.8295\n",
            "\n",
            "Epoch 00324 | Train Loss 1.4318 | Train Acc 0.3718 | Val Loss 1.5920 | Val Acc 0.2999 | Time(s) 71.8435\n",
            "\n",
            "Epoch 00325 | Train Loss 1.4287 | Train Acc 0.3686 | Val Loss 1.5915 | Val Acc 0.2993 | Time(s) 71.8692\n",
            "\n",
            "Epoch 00326 | Train Loss 1.4361 | Train Acc 0.3578 | Val Loss 1.5902 | Val Acc 0.3029 | Time(s) 71.8944\n",
            "\n",
            "Epoch 00327 | Train Loss 1.3922 | Train Acc 0.3622 | Val Loss 1.5897 | Val Acc 0.2993 | Time(s) 71.9160\n",
            "\n",
            "Epoch 00328 | Train Loss 1.4388 | Train Acc 0.3622 | Val Loss 1.5896 | Val Acc 0.2951 | Time(s) 71.9387\n",
            "\n",
            "Epoch 00329 | Train Loss 1.4281 | Train Acc 0.3554 | Val Loss 1.5892 | Val Acc 0.2957 | Time(s) 71.9617\n",
            "\n",
            "Epoch 00330 | Train Loss 1.4239 | Train Acc 0.3634 | Val Loss 1.5896 | Val Acc 0.2921 | Time(s) 71.9710\n",
            "\n",
            "Epoch 00331 | Train Loss 1.4227 | Train Acc 0.3722 | Val Loss 1.5897 | Val Acc 0.2939 | Time(s) 71.9933\n",
            "\n",
            "Epoch 00332 | Train Loss 1.4204 | Train Acc 0.3550 | Val Loss 1.5904 | Val Acc 0.2993 | Time(s) 72.0086\n",
            "\n",
            "Epoch 00333 | Train Loss 1.4100 | Train Acc 0.3682 | Val Loss 1.5910 | Val Acc 0.3005 | Time(s) 72.0229\n",
            "\n",
            "Epoch 00334 | Train Loss 1.4312 | Train Acc 0.3586 | Val Loss 1.5927 | Val Acc 0.3029 | Time(s) 72.0351\n",
            "\n",
            "Epoch 00335 | Train Loss 1.4130 | Train Acc 0.3722 | Val Loss 1.5947 | Val Acc 0.3029 | Time(s) 72.0480\n",
            "\n",
            "Epoch 00336 | Train Loss 1.3984 | Train Acc 0.3890 | Val Loss 1.5960 | Val Acc 0.3005 | Time(s) 72.0611\n",
            "\n",
            "Epoch 00337 | Train Loss 1.4054 | Train Acc 0.3746 | Val Loss 1.5967 | Val Acc 0.3005 | Time(s) 72.0738\n",
            "\n",
            "Epoch 00338 | Train Loss 1.4271 | Train Acc 0.3622 | Val Loss 1.5971 | Val Acc 0.2999 | Time(s) 72.0876\n",
            "\n",
            "Epoch 00339 | Train Loss 1.4183 | Train Acc 0.3674 | Val Loss 1.5971 | Val Acc 0.2987 | Time(s) 72.0976\n",
            "\n",
            "Epoch 00340 | Train Loss 1.4265 | Train Acc 0.3698 | Val Loss 1.5960 | Val Acc 0.2993 | Time(s) 72.0805\n",
            "\n",
            "Epoch 00341 | Train Loss 1.3897 | Train Acc 0.3722 | Val Loss 1.5945 | Val Acc 0.3023 | Time(s) 72.0792\n",
            "\n",
            "Epoch 00342 | Train Loss 1.4417 | Train Acc 0.3514 | Val Loss 1.5933 | Val Acc 0.3023 | Time(s) 72.0845\n",
            "\n",
            "Epoch 00343 | Train Loss 1.4195 | Train Acc 0.3730 | Val Loss 1.5924 | Val Acc 0.3029 | Time(s) 72.0747\n",
            "\n",
            "Epoch 00344 | Train Loss 1.4114 | Train Acc 0.3738 | Val Loss 1.5918 | Val Acc 0.3047 | Time(s) 72.0753\n",
            "\n",
            "Epoch 00345 | Train Loss 1.4016 | Train Acc 0.3654 | Val Loss 1.5915 | Val Acc 0.3041 | Time(s) 72.0743\n",
            "\n",
            "Epoch 00346 | Train Loss 1.4142 | Train Acc 0.3646 | Val Loss 1.5927 | Val Acc 0.3029 | Time(s) 72.0728\n",
            "\n",
            "Epoch 00347 | Train Loss 1.4174 | Train Acc 0.3646 | Val Loss 1.5944 | Val Acc 0.3035 | Time(s) 72.0641\n",
            "\n",
            "Epoch 00348 | Train Loss 1.4417 | Train Acc 0.3498 | Val Loss 1.5963 | Val Acc 0.2999 | Time(s) 72.0624\n",
            "\n",
            "Epoch 00349 | Train Loss 1.4326 | Train Acc 0.3626 | Val Loss 1.5983 | Val Acc 0.2987 | Time(s) 72.0616\n",
            "\n",
            "Epoch 00350 | Train Loss 1.3984 | Train Acc 0.3826 | Val Loss 1.5994 | Val Acc 0.2957 | Time(s) 72.0616\n",
            "\n",
            "Epoch 00351 | Train Loss 1.3952 | Train Acc 0.3774 | Val Loss 1.5999 | Val Acc 0.2951 | Time(s) 72.0730\n",
            "\n",
            "Epoch 00352 | Train Loss 1.4249 | Train Acc 0.3558 | Val Loss 1.5985 | Val Acc 0.2933 | Time(s) 72.0739\n",
            "\n",
            "Epoch 00353 | Train Loss 1.4080 | Train Acc 0.3694 | Val Loss 1.5970 | Val Acc 0.2963 | Time(s) 72.0739\n",
            "\n",
            "Epoch 00354 | Train Loss 1.4274 | Train Acc 0.3722 | Val Loss 1.5949 | Val Acc 0.2951 | Time(s) 72.0850\n",
            "\n",
            "Epoch 00355 | Train Loss 1.4072 | Train Acc 0.3710 | Val Loss 1.5933 | Val Acc 0.2957 | Time(s) 72.0957\n",
            "\n",
            "Epoch 00356 | Train Loss 1.4269 | Train Acc 0.3642 | Val Loss 1.5925 | Val Acc 0.2951 | Time(s) 72.1075\n",
            "\n",
            "Epoch 00357 | Train Loss 1.4259 | Train Acc 0.3678 | Val Loss 1.5915 | Val Acc 0.2999 | Time(s) 72.1171\n",
            "\n",
            "Epoch 00358 | Train Loss 1.4124 | Train Acc 0.3602 | Val Loss 1.5911 | Val Acc 0.3017 | Time(s) 72.1268\n",
            "\n",
            "Epoch 00359 | Train Loss 1.4155 | Train Acc 0.3558 | Val Loss 1.5909 | Val Acc 0.3029 | Time(s) 72.1374\n",
            "\n",
            "Epoch 00360 | Train Loss 1.4291 | Train Acc 0.3618 | Val Loss 1.5912 | Val Acc 0.3089 | Time(s) 72.1466\n",
            "\n",
            "Epoch 00361 | Train Loss 1.4147 | Train Acc 0.3570 | Val Loss 1.5927 | Val Acc 0.3041 | Time(s) 72.1457\n",
            "\n",
            "Epoch 00362 | Train Loss 1.4224 | Train Acc 0.3778 | Val Loss 1.5940 | Val Acc 0.3035 | Time(s) 72.1458\n",
            "\n",
            "Epoch 00363 | Train Loss 1.4235 | Train Acc 0.3670 | Val Loss 1.5950 | Val Acc 0.3047 | Time(s) 72.1561\n",
            "\n",
            "Epoch 00364 | Train Loss 1.4110 | Train Acc 0.3650 | Val Loss 1.5952 | Val Acc 0.3029 | Time(s) 72.1673\n",
            "\n",
            "Epoch 00365 | Train Loss 1.4033 | Train Acc 0.3630 | Val Loss 1.5962 | Val Acc 0.2993 | Time(s) 72.1778\n",
            "\n",
            "Epoch 00366 | Train Loss 1.4118 | Train Acc 0.3694 | Val Loss 1.5971 | Val Acc 0.2957 | Time(s) 72.1937\n",
            "\n",
            "Epoch 00367 | Train Loss 1.4108 | Train Acc 0.3718 | Val Loss 1.5972 | Val Acc 0.2945 | Time(s) 72.1836\n",
            "\n",
            "Epoch 00368 | Train Loss 1.4098 | Train Acc 0.3726 | Val Loss 1.5962 | Val Acc 0.2957 | Time(s) 72.1834\n",
            "\n",
            "Epoch 00369 | Train Loss 1.4414 | Train Acc 0.3622 | Val Loss 1.5949 | Val Acc 0.2939 | Time(s) 72.1860\n",
            "\n",
            "Epoch 00370 | Train Loss 1.4083 | Train Acc 0.3610 | Val Loss 1.5940 | Val Acc 0.2927 | Time(s) 72.1785\n",
            "\n",
            "Epoch 00371 | Train Loss 1.4017 | Train Acc 0.3730 | Val Loss 1.5935 | Val Acc 0.2981 | Time(s) 72.1772\n",
            "\n",
            "Epoch 00372 | Train Loss 1.4182 | Train Acc 0.3782 | Val Loss 1.5931 | Val Acc 0.3029 | Time(s) 72.1908\n",
            "\n",
            "Epoch 00373 | Train Loss 1.4105 | Train Acc 0.3742 | Val Loss 1.5936 | Val Acc 0.3047 | Time(s) 72.2004\n",
            "\n",
            "Epoch 00374 | Train Loss 1.4030 | Train Acc 0.3690 | Val Loss 1.5951 | Val Acc 0.3077 | Time(s) 72.2051\n",
            "\n",
            "Epoch 00375 | Train Loss 1.3935 | Train Acc 0.3778 | Val Loss 1.5966 | Val Acc 0.3065 | Time(s) 72.1977\n",
            "\n",
            "Epoch 00376 | Train Loss 1.4128 | Train Acc 0.3750 | Val Loss 1.5963 | Val Acc 0.3029 | Time(s) 72.1955\n",
            "\n",
            "Epoch 00377 | Train Loss 1.4123 | Train Acc 0.3686 | Val Loss 1.5957 | Val Acc 0.3023 | Time(s) 72.1789\n",
            "\n",
            "Epoch 00378 | Train Loss 1.4133 | Train Acc 0.3706 | Val Loss 1.5953 | Val Acc 0.3011 | Time(s) 72.1618\n",
            "\n",
            "Epoch 00379 | Train Loss 1.3848 | Train Acc 0.3746 | Val Loss 1.5946 | Val Acc 0.2999 | Time(s) 72.1443\n",
            "\n",
            "Epoch 00380 | Train Loss 1.4153 | Train Acc 0.3634 | Val Loss 1.5947 | Val Acc 0.2987 | Time(s) 72.1276\n",
            "\n",
            "Epoch 00381 | Train Loss 1.4022 | Train Acc 0.3722 | Val Loss 1.5958 | Val Acc 0.2957 | Time(s) 72.1096\n",
            "\n",
            "Epoch 00382 | Train Loss 1.3985 | Train Acc 0.3726 | Val Loss 1.5971 | Val Acc 0.2951 | Time(s) 72.0934\n",
            "\n",
            "Epoch 00383 | Train Loss 1.4109 | Train Acc 0.3706 | Val Loss 1.5986 | Val Acc 0.2927 | Time(s) 72.0758\n",
            "\n",
            "Epoch 00384 | Train Loss 1.4190 | Train Acc 0.3698 | Val Loss 1.5998 | Val Acc 0.2921 | Time(s) 72.0586\n",
            "\n",
            "Epoch 00385 | Train Loss 1.4003 | Train Acc 0.3754 | Val Loss 1.6002 | Val Acc 0.2921 | Time(s) 72.0495\n",
            "\n",
            "Epoch 00386 | Train Loss 1.3960 | Train Acc 0.3814 | Val Loss 1.6001 | Val Acc 0.2909 | Time(s) 72.0435\n",
            "\n",
            "Epoch 00387 | Train Loss 1.4038 | Train Acc 0.3754 | Val Loss 1.6001 | Val Acc 0.2933 | Time(s) 72.0395\n",
            "\n",
            "Epoch 00388 | Train Loss 1.4042 | Train Acc 0.3578 | Val Loss 1.5994 | Val Acc 0.2921 | Time(s) 72.0330\n",
            "\n",
            "Epoch 00389 | Train Loss 1.4139 | Train Acc 0.3650 | Val Loss 1.5984 | Val Acc 0.2957 | Time(s) 72.0283\n",
            "\n",
            "Epoch 00390 | Train Loss 1.4035 | Train Acc 0.3798 | Val Loss 1.5980 | Val Acc 0.2951 | Time(s) 72.0284\n",
            "\n",
            "Epoch 00391 | Train Loss 1.4068 | Train Acc 0.3802 | Val Loss 1.5978 | Val Acc 0.2933 | Time(s) 72.0224\n",
            "\n",
            "Epoch 00392 | Train Loss 1.3994 | Train Acc 0.3814 | Val Loss 1.5979 | Val Acc 0.2909 | Time(s) 72.0144\n",
            "\n",
            "Epoch 00393 | Train Loss 1.4038 | Train Acc 0.3594 | Val Loss 1.5984 | Val Acc 0.2897 | Time(s) 72.0136\n",
            "\n",
            "Epoch 00394 | Train Loss 1.3811 | Train Acc 0.3850 | Val Loss 1.5989 | Val Acc 0.2909 | Time(s) 72.0054\n",
            "\n",
            "Epoch 00395 | Train Loss 1.4225 | Train Acc 0.3658 | Val Loss 1.5999 | Val Acc 0.2933 | Time(s) 72.0217\n",
            "\n",
            "Epoch 00396 | Train Loss 1.4164 | Train Acc 0.3694 | Val Loss 1.6010 | Val Acc 0.2963 | Time(s) 72.0320\n",
            "\n",
            "Epoch 00397 | Train Loss 1.3999 | Train Acc 0.3682 | Val Loss 1.6021 | Val Acc 0.3041 | Time(s) 72.0494\n",
            "\n",
            "Epoch 00398 | Train Loss 1.3750 | Train Acc 0.3786 | Val Loss 1.6022 | Val Acc 0.3041 | Time(s) 72.0589\n",
            "\n",
            "Epoch 00399 | Train Loss 1.3910 | Train Acc 0.3718 | Val Loss 1.6020 | Val Acc 0.3059 | Time(s) 72.0641\n",
            "\n",
            "Epoch 00400 | Train Loss 1.3974 | Train Acc 0.3638 | Val Loss 1.6015 | Val Acc 0.3035 | Time(s) 72.0580\n",
            "\n",
            "Epoch 00401 | Train Loss 1.3941 | Train Acc 0.3686 | Val Loss 1.6007 | Val Acc 0.2981 | Time(s) 72.0637\n",
            "\n",
            "Epoch 00402 | Train Loss 1.3993 | Train Acc 0.3726 | Val Loss 1.5997 | Val Acc 0.2963 | Time(s) 72.0608\n",
            "\n",
            "Epoch 00403 | Train Loss 1.3874 | Train Acc 0.3786 | Val Loss 1.5991 | Val Acc 0.2951 | Time(s) 72.0711\n",
            "\n",
            "Epoch 00404 | Train Loss 1.3921 | Train Acc 0.3658 | Val Loss 1.5991 | Val Acc 0.2909 | Time(s) 72.0725\n",
            "\n",
            "Epoch 00405 | Train Loss 1.3913 | Train Acc 0.3842 | Val Loss 1.5994 | Val Acc 0.2891 | Time(s) 72.0646\n",
            "\n",
            "Epoch 00406 | Train Loss 1.3895 | Train Acc 0.3802 | Val Loss 1.6004 | Val Acc 0.2897 | Time(s) 72.0682\n",
            "\n",
            "Epoch 00407 | Train Loss 1.3838 | Train Acc 0.3766 | Val Loss 1.6015 | Val Acc 0.2891 | Time(s) 72.0697\n",
            "\n",
            "Epoch 00408 | Train Loss 1.3945 | Train Acc 0.3794 | Val Loss 1.6025 | Val Acc 0.2921 | Time(s) 72.0618\n",
            "\n",
            "Epoch 00409 | Train Loss 1.3824 | Train Acc 0.3838 | Val Loss 1.6032 | Val Acc 0.2951 | Time(s) 72.0459\n",
            "\n",
            "Epoch 00410 | Train Loss 1.4032 | Train Acc 0.3710 | Val Loss 1.6035 | Val Acc 0.2969 | Time(s) 72.0436\n",
            "\n",
            "Epoch 00411 | Train Loss 1.3961 | Train Acc 0.3678 | Val Loss 1.6038 | Val Acc 0.3011 | Time(s) 72.0503\n",
            "\n",
            "Epoch 00412 | Train Loss 1.4138 | Train Acc 0.3594 | Val Loss 1.6048 | Val Acc 0.3005 | Time(s) 72.0471\n",
            "\n",
            "Epoch 00413 | Train Loss 1.3985 | Train Acc 0.3642 | Val Loss 1.6056 | Val Acc 0.2999 | Time(s) 72.0279\n",
            "\n",
            "Epoch 00414 | Train Loss 1.4144 | Train Acc 0.3670 | Val Loss 1.6053 | Val Acc 0.3059 | Time(s) 72.0126\n",
            "\n",
            "Epoch 00415 | Train Loss 1.3946 | Train Acc 0.3806 | Val Loss 1.6045 | Val Acc 0.3053 | Time(s) 72.0126\n",
            "\n",
            "Epoch 00416 | Train Loss 1.3961 | Train Acc 0.3742 | Val Loss 1.6034 | Val Acc 0.3047 | Time(s) 72.0129\n",
            "\n",
            "Epoch 00417 | Train Loss 1.3909 | Train Acc 0.3698 | Val Loss 1.6024 | Val Acc 0.3023 | Time(s) 72.0061\n",
            "\n",
            "Epoch 00418 | Train Loss 1.4140 | Train Acc 0.3714 | Val Loss 1.6025 | Val Acc 0.3011 | Time(s) 72.0059\n",
            "\n",
            "Epoch 00419 | Train Loss 1.3990 | Train Acc 0.3638 | Val Loss 1.6027 | Val Acc 0.2963 | Time(s) 72.0013\n",
            "\n",
            "Epoch 00420 | Train Loss 1.3803 | Train Acc 0.3926 | Val Loss 1.6037 | Val Acc 0.2933 | Time(s) 72.0002\n",
            "\n",
            "Epoch 00421 | Train Loss 1.4039 | Train Acc 0.3810 | Val Loss 1.6049 | Val Acc 0.2927 | Time(s) 71.9909\n",
            "\n",
            "Epoch 00422 | Train Loss 1.3782 | Train Acc 0.3822 | Val Loss 1.6057 | Val Acc 0.2921 | Time(s) 71.9769\n",
            "\n",
            "Epoch 00423 | Train Loss 1.3982 | Train Acc 0.3726 | Val Loss 1.6060 | Val Acc 0.2909 | Time(s) 71.9827\n",
            "\n",
            "Epoch 00424 | Train Loss 1.3951 | Train Acc 0.3758 | Val Loss 1.6060 | Val Acc 0.2909 | Time(s) 71.9820\n",
            "\n",
            "Epoch 00425 | Train Loss 1.3900 | Train Acc 0.3658 | Val Loss 1.6053 | Val Acc 0.2951 | Time(s) 71.9823\n",
            "\n",
            "Epoch 00426 | Train Loss 1.4029 | Train Acc 0.3614 | Val Loss 1.6044 | Val Acc 0.2945 | Time(s) 71.9791\n",
            "\n",
            "Epoch 00427 | Train Loss 1.3880 | Train Acc 0.3878 | Val Loss 1.6035 | Val Acc 0.3023 | Time(s) 71.9739\n",
            "\n",
            "Epoch 00428 | Train Loss 1.3850 | Train Acc 0.3694 | Val Loss 1.6028 | Val Acc 0.3035 | Time(s) 71.9726\n",
            "\n",
            "Epoch 00429 | Train Loss 1.3874 | Train Acc 0.3826 | Val Loss 1.6025 | Val Acc 0.3017 | Time(s) 71.9724\n",
            "\n",
            "Epoch 00430 | Train Loss 1.3968 | Train Acc 0.3798 | Val Loss 1.6014 | Val Acc 0.3005 | Time(s) 71.9780\n",
            "\n",
            "Epoch 00431 | Train Loss 1.3726 | Train Acc 0.3838 | Val Loss 1.6007 | Val Acc 0.3005 | Time(s) 71.9724\n",
            "\n",
            "Epoch 00432 | Train Loss 1.3982 | Train Acc 0.3738 | Val Loss 1.6005 | Val Acc 0.2987 | Time(s) 71.9749\n",
            "\n",
            "Epoch 00433 | Train Loss 1.4134 | Train Acc 0.3786 | Val Loss 1.6006 | Val Acc 0.2993 | Time(s) 71.9749\n",
            "\n",
            "Epoch 00434 | Train Loss 1.3895 | Train Acc 0.3750 | Val Loss 1.6016 | Val Acc 0.2993 | Time(s) 71.9805\n",
            "\n",
            "Epoch 00435 | Train Loss 1.3989 | Train Acc 0.3798 | Val Loss 1.6026 | Val Acc 0.2975 | Time(s) 71.9811\n",
            "\n",
            "Epoch 00436 | Train Loss 1.3724 | Train Acc 0.3906 | Val Loss 1.6033 | Val Acc 0.2969 | Time(s) 71.9682\n",
            "\n",
            "Epoch 00437 | Train Loss 1.3742 | Train Acc 0.3830 | Val Loss 1.6037 | Val Acc 0.2987 | Time(s) 71.9588\n",
            "\n",
            "Epoch 00438 | Train Loss 1.3593 | Train Acc 0.3930 | Val Loss 1.6039 | Val Acc 0.2999 | Time(s) 71.9593\n",
            "\n",
            "Epoch 00439 | Train Loss 1.4180 | Train Acc 0.3630 | Val Loss 1.6037 | Val Acc 0.3005 | Time(s) 71.9653\n",
            "\n",
            "Epoch 00440 | Train Loss 1.4094 | Train Acc 0.3802 | Val Loss 1.6033 | Val Acc 0.2993 | Time(s) 71.9666\n",
            "\n",
            "Epoch 00441 | Train Loss 1.4172 | Train Acc 0.3746 | Val Loss 1.6029 | Val Acc 0.2993 | Time(s) 71.9781\n",
            "\n",
            "Epoch 00442 | Train Loss 1.3933 | Train Acc 0.3750 | Val Loss 1.6030 | Val Acc 0.2993 | Time(s) 71.9504\n",
            "\n",
            "Epoch 00443 | Train Loss 1.3923 | Train Acc 0.3666 | Val Loss 1.6032 | Val Acc 0.2999 | Time(s) 71.9223\n",
            "\n",
            "Epoch 00444 | Train Loss 1.3918 | Train Acc 0.3742 | Val Loss 1.6038 | Val Acc 0.2999 | Time(s) 71.9084\n",
            "\n",
            "Epoch 00445 | Train Loss 1.3992 | Train Acc 0.3782 | Val Loss 1.6042 | Val Acc 0.3017 | Time(s) 71.9058\n",
            "\n",
            "Epoch 00446 | Train Loss 1.3823 | Train Acc 0.3814 | Val Loss 1.6046 | Val Acc 0.3011 | Time(s) 71.9039\n",
            "\n",
            "Epoch 00447 | Train Loss 1.4020 | Train Acc 0.3742 | Val Loss 1.6050 | Val Acc 0.2981 | Time(s) 71.9013\n",
            "\n",
            "Epoch 00448 | Train Loss 1.4051 | Train Acc 0.3750 | Val Loss 1.6050 | Val Acc 0.2957 | Time(s) 71.9116\n",
            "\n",
            "Epoch 00449 | Train Loss 1.3857 | Train Acc 0.3730 | Val Loss 1.6048 | Val Acc 0.2951 | Time(s) 71.9131\n",
            "\n",
            "Epoch 00450 | Train Loss 1.3790 | Train Acc 0.3734 | Val Loss 1.6048 | Val Acc 0.2963 | Time(s) 71.9079\n",
            "\n",
            "Epoch 00451 | Train Loss 1.3835 | Train Acc 0.3862 | Val Loss 1.6043 | Val Acc 0.2951 | Time(s) 71.9004\n",
            "\n",
            "Epoch 00452 | Train Loss 1.4061 | Train Acc 0.3706 | Val Loss 1.6036 | Val Acc 0.3011 | Time(s) 71.8885\n",
            "\n",
            "Epoch 00453 | Train Loss 1.4135 | Train Acc 0.3706 | Val Loss 1.6030 | Val Acc 0.2999 | Time(s) 71.8932\n",
            "\n",
            "Epoch 00454 | Train Loss 1.3984 | Train Acc 0.3738 | Val Loss 1.6030 | Val Acc 0.3011 | Time(s) 71.8970\n",
            "\n",
            "Epoch 00455 | Train Loss 1.3849 | Train Acc 0.3882 | Val Loss 1.6028 | Val Acc 0.3023 | Time(s) 71.8994\n",
            "\n",
            "Epoch 00456 | Train Loss 1.4089 | Train Acc 0.3722 | Val Loss 1.6031 | Val Acc 0.3005 | Time(s) 71.8783\n",
            "\n",
            "Epoch 00457 | Train Loss 1.3851 | Train Acc 0.3726 | Val Loss 1.6036 | Val Acc 0.3059 | Time(s) 71.8937\n",
            "\n",
            "Epoch 00458 | Train Loss 1.4014 | Train Acc 0.3758 | Val Loss 1.6043 | Val Acc 0.3005 | Time(s) 71.9045\n",
            "\n",
            "Epoch 00459 | Train Loss 1.4125 | Train Acc 0.3682 | Val Loss 1.6051 | Val Acc 0.3011 | Time(s) 71.9204\n",
            "\n",
            "Epoch 00460 | Train Loss 1.3908 | Train Acc 0.3734 | Val Loss 1.6052 | Val Acc 0.3017 | Time(s) 71.9233\n",
            "\n",
            "Epoch 00461 | Train Loss 1.3929 | Train Acc 0.3778 | Val Loss 1.6047 | Val Acc 0.2999 | Time(s) 71.9341\n",
            "\n",
            "Epoch 00462 | Train Loss 1.4012 | Train Acc 0.3766 | Val Loss 1.6038 | Val Acc 0.2999 | Time(s) 71.9448\n",
            "\n",
            "Epoch 00463 | Train Loss 1.3938 | Train Acc 0.3734 | Val Loss 1.6026 | Val Acc 0.2945 | Time(s) 71.9328\n",
            "\n",
            "Epoch 00464 | Train Loss 1.3862 | Train Acc 0.3814 | Val Loss 1.6016 | Val Acc 0.2945 | Time(s) 71.9207\n",
            "\n",
            "Epoch 00465 | Train Loss 1.3718 | Train Acc 0.3838 | Val Loss 1.6014 | Val Acc 0.2927 | Time(s) 71.9158\n",
            "\n",
            "Epoch 00466 | Train Loss 1.3854 | Train Acc 0.3822 | Val Loss 1.6009 | Val Acc 0.2915 | Time(s) 71.9081\n",
            "\n",
            "Epoch 00467 | Train Loss 1.4172 | Train Acc 0.3722 | Val Loss 1.6001 | Val Acc 0.2915 | Time(s) 71.8915\n",
            "\n",
            "Epoch 00468 | Train Loss 1.3902 | Train Acc 0.3730 | Val Loss 1.5996 | Val Acc 0.2963 | Time(s) 71.8685\n",
            "\n",
            "Epoch 00469 | Train Loss 1.3988 | Train Acc 0.3654 | Val Loss 1.5994 | Val Acc 0.2981 | Time(s) 71.8654\n",
            "\n",
            "Epoch 00470 | Train Loss 1.3972 | Train Acc 0.3658 | Val Loss 1.5998 | Val Acc 0.2987 | Time(s) 71.8588\n",
            "\n",
            "Epoch 00471 | Train Loss 1.4087 | Train Acc 0.3778 | Val Loss 1.6002 | Val Acc 0.3023 | Time(s) 71.8522\n",
            "\n",
            "Epoch 00472 | Train Loss 1.3863 | Train Acc 0.3822 | Val Loss 1.6011 | Val Acc 0.3047 | Time(s) 71.8510\n",
            "\n",
            "Epoch 00473 | Train Loss 1.3819 | Train Acc 0.3774 | Val Loss 1.6022 | Val Acc 0.3053 | Time(s) 71.8475\n",
            "\n",
            "Epoch 00474 | Train Loss 1.4001 | Train Acc 0.3782 | Val Loss 1.6039 | Val Acc 0.3041 | Time(s) 71.8462\n",
            "\n",
            "Epoch 00475 | Train Loss 1.3952 | Train Acc 0.3666 | Val Loss 1.6046 | Val Acc 0.3005 | Time(s) 71.8502\n",
            "\n",
            "Epoch 00476 | Train Loss 1.3856 | Train Acc 0.3702 | Val Loss 1.6042 | Val Acc 0.2999 | Time(s) 71.8480\n",
            "\n",
            "Epoch 00477 | Train Loss 1.3866 | Train Acc 0.3774 | Val Loss 1.6036 | Val Acc 0.2999 | Time(s) 71.8479\n",
            "\n",
            "Epoch 00478 | Train Loss 1.3826 | Train Acc 0.3982 | Val Loss 1.6031 | Val Acc 0.2963 | Time(s) 71.8457\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
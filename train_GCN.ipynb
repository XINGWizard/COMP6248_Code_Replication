{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_GCN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhNZda4n0L2w",
        "outputId": "1897d2e1-0573-4a56-85de-2f944b7407d4"
      },
      "source": [
        "!pip install dgl-0.3-cp37-cp37m-manylinux1_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dgl==0.3 from file:///content/dgl-0.3-cp37-cp37m-manylinux1_x86_64.whl in /usr/local/lib/python3.7/dist-packages (0.3)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl==0.3) (2.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl==0.3) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl==0.3) (1.4.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.1->dgl==0.3) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ORVos1r2fkL",
        "outputId": "3c863d31-8b44-45f3-fa1f-75cdc5e22f7d"
      },
      "source": [
        "!pip uninstall -y networkx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling networkx-2.5:\n",
            "  Successfully uninstalled networkx-2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgDUmm1b2sQi",
        "outputId": "540f0962-aa9b-47f2-a5cf-0e85202b0dbc"
      },
      "source": [
        "!pip install networkx-2.5-py3-none-any.whl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./networkx-2.5-py3-none-any.whl\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx==2.5) (4.4.2)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.7 which is incompatible.\u001b[0m\n",
            "Installing collected packages: networkx\n",
            "Successfully installed networkx-2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlK1PyE32uHC",
        "outputId": "399f0655-4058-4d7b-ec39-a1dac373fa00"
      },
      "source": [
        "!pip uninstall -y imgaug"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling imgaug-0.2.7:\n",
            "  Successfully uninstalled imgaug-0.2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzNH8UIa2z_6",
        "outputId": "9b6e7d43-27db-4fa9-b290-271ca12dbeb0"
      },
      "source": [
        "!pip install imgaug-0.2.7-py3-none-any.whl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./imgaug-0.2.7-py3-none-any.whl\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.7) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.7) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.7) (7.1.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.7) (1.7.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.7) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.7) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.7) (1.4.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.7) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.7) (2.5)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.7) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.2.7) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.2.7) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.2.7) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.2.7) (2.8.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug==0.2.7) (4.4.2)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.7 which is incompatible.\u001b[0m\n",
            "Installing collected packages: imgaug\n",
            "Successfully installed imgaug-0.2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoYFqubT24ia",
        "outputId": "0c38b427-6626-42f0-8c2c-cdbf28af9888"
      },
      "source": [
        "!pip install tensorboardX-2.1-py2.py3-none-any.whl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX==2.1 from file:///content/tensorboardX-2.1-py2.py3-none-any.whl in /usr/local/lib/python3.7/dist-packages (2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.1) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.1) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.1) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX==2.1) (56.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb75C44p26Iy",
        "outputId": "105f3848-59d5-4db6-c144-d53a097c5568"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M4mCvaQ3NOs"
      },
      "source": [
        "import os\n",
        "path=\"/content/drive/MyDrive/Others\"\n",
        "#path=\"/content/\"\n",
        "os.chdir(path)\n",
        "os.listdir(path)\n",
        "!python utils.py\n",
        "!python utils_data.py\n",
        "!python utils_layers.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkMQAl8r3T7U"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "\n",
        "import dgl.init\n",
        "import numpy as np\n",
        "import tensorboardX\n",
        "import torch as th\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import utils_data\n",
        "from utils_layers import GCNNet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PURu7tYf3ivt"
      },
      "source": [
        "args={'model':'GCN_TwoLayers',\n",
        "  'dataset':'wisconsin',\n",
        "  'num_hidden':32,\n",
        "  'num_heads_layer_one':1,\n",
        "  'num_heads_layer_two':1,\n",
        "  'dropout_rate':0.5,\n",
        "  'learning_rate':0.05,\n",
        "  'weight_decay_layer_one':5e-06,\n",
        "  'weight_decay_layer_two':5e-06,\n",
        "  'num_epochs_patience':100,\n",
        "  'num_epochs_max':5000,\n",
        "  'run_id':0,\n",
        "  'dataset_split':'splits/wisconsin_split_0.6_0.2_0.npz',\n",
        "  'learning_rate_decay_patience':50,\n",
        "  'learning_rate_decay_factor':0.8\n",
        "   }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56t9631o3Zks",
        "outputId": "62da128e-cb38-41c4-e8e6-e585b05ecdea"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    record = pd.DataFrame(columns=['Train Loss', 'Val Loss', 'Train Acc', 'Val Acc'])\n",
        "    '''\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--dataset', type=str)\n",
        "    parser.add_argument('--num_hidden', type=int)\n",
        "    parser.add_argument('--num_heads_layer_one', type=int)\n",
        "    parser.add_argument('--num_heads_layer_two', type=int)\n",
        "    parser.add_argument('--dropout_rate', type=float)\n",
        "    parser.add_argument('--learning_rate', type=float)\n",
        "    parser.add_argument('--weight_decay_layer_one', type=float)\n",
        "    parser.add_argument('--weight_decay_layer_two', type=float)\n",
        "    parser.add_argument('--num_epochs_patience', type=int, default=100)\n",
        "    parser.add_argument('--num_epochs_max', type=int, default=5000)\n",
        "    parser.add_argument('--run_id', type=str)\n",
        "    parser.add_argument('--dataset_split', type=str)\n",
        "    parser.add_argument('--learning_rate_decay_patience', type=int, default=50)\n",
        "    parser.add_argument('--learning_rate_decay_factor', type=float, default=0.8)\n",
        "    args = parser.parse_args()\n",
        "    vars(args)['model'] = 'GCN_TwoLayers'\n",
        "    '''\n",
        "\n",
        "    if args[\"dataset_split\"] == 'jknet':\n",
        "        g, features, labels, train_mask, val_mask, test_mask, num_features, num_labels = utils_data.load_data(\n",
        "            args['dataset'], None, 0.6, 0.2)\n",
        "    else:\n",
        "        g, features, labels, train_mask, val_mask, test_mask, num_features, num_labels = utils_data.load_data(\n",
        "            args['dataset'], args['dataset_split'])\n",
        "\n",
        "    g.set_n_initializer(dgl.init.zero_initializer)\n",
        "    g.set_e_initializer(dgl.init.zero_initializer)\n",
        "\n",
        "    net = GCNNet(num_input_features=num_features, num_output_classes=num_labels, num_hidden=args['num_hidden'],\n",
        "            dropout_rate=args['dropout_rate'],num_heads_layer_one=args['num_heads_layer_one'], num_heads_layer_two=args['num_heads_layer_two'])\n",
        "\n",
        "    optimizer = th.optim.Adam([{'params': net.gcn1.parameters(), 'weight_decay': args['weight_decay_layer_one']},\n",
        "                               {'params': net.gcn2.parameters(), 'weight_decay': args['weight_decay_layer_two']}],\n",
        "                              lr=args['learning_rate'])\n",
        "    learning_rate_scheduler = th.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=args['learning_rate_decay_factor'],patience=args['learning_rate_decay_patience'])\n",
        "    #writer = tensorboardX.SummaryWriter(logdir=f'runs/{args['model']}_{args['run_id']}')\n",
        "    writer = tensorboardX.SummaryWriter(logdir=f'runs/{args[\"model\"]}_{args[\"run_id\"]}')\n",
        "\n",
        "    features = features\n",
        "    labels = labels\n",
        "    train_mask = train_mask\n",
        "    val_mask = val_mask\n",
        "    test_mask = test_mask\n",
        "\n",
        "    # Adapted from https://github.com/PetarV-/GAT/blob/master/execute_cora.py\n",
        "    patience = args['num_epochs_patience']\n",
        "    vlss_mn = np.inf\n",
        "    vacc_mx = 0.0\n",
        "    vacc_early_model = None\n",
        "    vlss_early_model = None\n",
        "    state_dict_early_model = None\n",
        "    curr_step = 0\n",
        "\n",
        "    # Adapted from https://docs.dgl.ai/tutorials/models/1_gnn/1_gcn.html\n",
        "    dur = []\n",
        "    test_time = 0.0\n",
        "    for epoch in range(500):\n",
        "        t0 = time.time()\n",
        "\n",
        "        net.train()\n",
        "        train_logits = net(g, features)\n",
        "        train_logp = F.log_softmax(train_logits, 1)\n",
        "        train_loss = F.nll_loss(train_logp[train_mask], labels[train_mask])\n",
        "        train_pred = train_logp.argmax(dim=1)\n",
        "        train_acc = th.eq(train_pred[train_mask], labels[train_mask]).float().mean().item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        net.eval()\n",
        "        with th.no_grad():\n",
        "            val_logits = net(g, features)\n",
        "            val_logp = F.log_softmax(val_logits, 1)\n",
        "            val_loss = F.nll_loss(val_logp[val_mask], labels[val_mask]).item()\n",
        "            val_pred = val_logp.argmax(dim=1)\n",
        "            val_acc = th.eq(val_pred[val_mask], labels[val_mask]).float().mean().item()\n",
        "\n",
        "        learning_rate_scheduler.step(val_loss)\n",
        "\n",
        "        dur.append(time.time() - t0)\n",
        "\n",
        "        print(\n",
        "            \"Epoch {:05d} | Train Loss {:.4f} | Train Acc {:.4f} | Val Loss {:.4f} | Val Acc {:.4f} | Time(s) {:.4f}\".format(\n",
        "                epoch, train_loss.item(), train_acc, val_loss, val_acc, sum(dur) / len(dur)))\n",
        "\n",
        "        writer.add_scalar('Train Loss', train_loss.item(), epoch)\n",
        "        writer.add_scalar('Val Loss', val_loss, epoch)\n",
        "        writer.add_scalar('Train Acc', train_acc, epoch)\n",
        "        writer.add_scalar('Val Acc', val_acc, epoch)\n",
        "\n",
        "        test_time += (sum(dur) / len(dur))\n",
        "        new={'Train Loss':train_loss.item(),'Val Loss':val_loss,'Train Acc':train_acc,'Val Acc':val_acc}\n",
        "\n",
        "        record=record.append(new,ignore_index=True)   # ignore_index=True,表示不按原来的索引，从0开始自动递增\n",
        "\n",
        "        # Adapted from https://github.com/PetarV-/GAT/blob/master/execute_cora.py\n",
        "        if val_acc >= vacc_mx or val_loss <= vlss_mn:\n",
        "            if val_acc >= vacc_mx and val_loss <= vlss_mn:\n",
        "                vacc_early_model = val_acc\n",
        "                vlss_early_model = val_loss\n",
        "                state_dict_early_model = net.state_dict()\n",
        "            vacc_mx = np.max((val_acc, vacc_mx))\n",
        "            vlss_mn = np.min((val_loss, vlss_mn))\n",
        "            curr_step = 0\n",
        "        else:\n",
        "            curr_step += 1\n",
        "            if curr_step >= patience:\n",
        "                print(\"\")\n",
        "\n",
        "    #drive.mount(\"/content\")\n",
        "    \n",
        "    record.to_csv('Test_Result/test_{model}_{dataset}_{rec_time}.csv'.format(model=args[\"model\"], dataset=args[\"dataset\"], rec_time=test_time))\n",
        "    print(test_time)\n",
        "\n",
        "    net.load_state_dict(state_dict_early_model)\n",
        "    net.eval()\n",
        "    with th.no_grad():\n",
        "        test_logits = net(g, features)\n",
        "        test_logp = F.log_softmax(test_logits, 1)\n",
        "        test_loss = F.nll_loss(test_logp[test_mask], labels[test_mask]).item()\n",
        "        test_pred = test_logp.argmax(dim=1)\n",
        "        test_acc = th.eq(test_pred[test_mask], labels[test_mask]).float().mean().item()\n",
        "        test_hidden_features = net.gcn1(g, features).cpu().numpy()\n",
        "\n",
        "        final_train_pred = test_pred[train_mask].cpu().numpy()\n",
        "        final_val_pred = test_pred[val_mask].cpu().numpy()\n",
        "        final_test_pred = test_pred[test_mask].cpu().numpy()\n",
        "    '''\n",
        "    results_dict = vars(args)\n",
        "    results_dict['test_loss'] = test_loss\n",
        "    results_dict['test_acc'] = test_acc\n",
        "    results_dict['actual_epochs'] = 1 + epoch\n",
        "    results_dict['val_acc_max'] = vacc_mx\n",
        "    results_dict['val_loss_min'] = vlss_mn\n",
        "    results_dict['total_time'] = sum(dur)\n",
        "    with open(os.path.join('runs', f'{args.model}_{args.run_id}_results.txt'), 'w') as outfile:\n",
        "        outfile.write(json.dumps(results_dict) + '\\n')\n",
        "    np.savez_compressed(os.path.join('runs', f'{args.model}_{args.run_id}_hidden_features.npz'),\n",
        "                        hidden_features=test_hidden_features)\n",
        "    np.savez_compressed(os.path.join('runs', f'{args.model}_{args.run_id}_final_train_predictions.npz'),\n",
        "                        final_train_predictions=final_train_pred)\n",
        "    np.savez_compressed(os.path.join('runs', f'{args.model}_{args.run_id}_final_val_predictions.npz'),\n",
        "                        final_val_predictions=final_val_pred)\n",
        "    np.savez_compressed(os.path.join('runs', f'{args.model}_{args.run_id}_final_test_predictions.npz'),\n",
        "                        final_test_predictions=final_test_pred)\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test1\n",
            "test4\n",
            "test5\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "DGLGraph(num_nodes=251, num_edges=750,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "<class 'dgl.graph.DGLGraph'>\n",
            "<class 'dgl.graph.DGLGraph'>\n",
            "test_end\n",
            "<class 'dgl.graph.DGLGraph'>\n",
            "Epoch 00000 | Train Loss 1.6094 | Train Acc 0.2000 | Val Loss 1.5438 | Val Acc 0.5500 | Time(s) 0.0189\n",
            "Epoch 00001 | Train Loss 1.5360 | Train Acc 0.3083 | Val Loss 1.4421 | Val Acc 0.5000 | Time(s) 0.0180\n",
            "Epoch 00002 | Train Loss 1.4130 | Train Acc 0.4500 | Val Loss 1.3479 | Val Acc 0.5000 | Time(s) 0.0175\n",
            "Epoch 00003 | Train Loss 1.3558 | Train Acc 0.5333 | Val Loss 1.3005 | Val Acc 0.5250 | Time(s) 0.0177\n",
            "Epoch 00004 | Train Loss 1.2979 | Train Acc 0.4500 | Val Loss 1.2967 | Val Acc 0.5250 | Time(s) 0.0177\n",
            "Epoch 00005 | Train Loss 1.2571 | Train Acc 0.4500 | Val Loss 1.3031 | Val Acc 0.5375 | Time(s) 0.0175\n",
            "Epoch 00006 | Train Loss 1.2256 | Train Acc 0.5500 | Val Loss 1.2906 | Val Acc 0.5125 | Time(s) 0.0174\n",
            "Epoch 00007 | Train Loss 1.1795 | Train Acc 0.5500 | Val Loss 1.2694 | Val Acc 0.5000 | Time(s) 0.0173\n",
            "Epoch 00008 | Train Loss 1.1686 | Train Acc 0.5333 | Val Loss 1.2390 | Val Acc 0.5125 | Time(s) 0.0174\n",
            "Epoch 00009 | Train Loss 1.0572 | Train Acc 0.5500 | Val Loss 1.2158 | Val Acc 0.5000 | Time(s) 0.0183\n",
            "Epoch 00010 | Train Loss 1.0744 | Train Acc 0.6083 | Val Loss 1.1999 | Val Acc 0.5000 | Time(s) 0.0182\n",
            "Epoch 00011 | Train Loss 1.0435 | Train Acc 0.6083 | Val Loss 1.1879 | Val Acc 0.5000 | Time(s) 0.0184\n",
            "Epoch 00012 | Train Loss 1.0385 | Train Acc 0.6083 | Val Loss 1.1752 | Val Acc 0.5000 | Time(s) 0.0184\n",
            "Epoch 00013 | Train Loss 0.9856 | Train Acc 0.6333 | Val Loss 1.1680 | Val Acc 0.5125 | Time(s) 0.0183\n",
            "Epoch 00014 | Train Loss 0.9992 | Train Acc 0.6333 | Val Loss 1.1647 | Val Acc 0.5125 | Time(s) 0.0181\n",
            "Epoch 00015 | Train Loss 0.9938 | Train Acc 0.6500 | Val Loss 1.1689 | Val Acc 0.5125 | Time(s) 0.0180\n",
            "Epoch 00016 | Train Loss 0.9239 | Train Acc 0.6583 | Val Loss 1.1715 | Val Acc 0.5125 | Time(s) 0.0180\n",
            "Epoch 00017 | Train Loss 0.8967 | Train Acc 0.6333 | Val Loss 1.1816 | Val Acc 0.5250 | Time(s) 0.0183\n",
            "Epoch 00018 | Train Loss 0.8782 | Train Acc 0.6583 | Val Loss 1.1786 | Val Acc 0.5375 | Time(s) 0.0181\n",
            "Epoch 00019 | Train Loss 0.8593 | Train Acc 0.6500 | Val Loss 1.1803 | Val Acc 0.5375 | Time(s) 0.0182\n",
            "Epoch 00020 | Train Loss 0.8385 | Train Acc 0.6917 | Val Loss 1.1793 | Val Acc 0.5250 | Time(s) 0.0181\n",
            "Epoch 00021 | Train Loss 0.8666 | Train Acc 0.6833 | Val Loss 1.1769 | Val Acc 0.5000 | Time(s) 0.0181\n",
            "Epoch 00022 | Train Loss 0.8195 | Train Acc 0.6583 | Val Loss 1.1779 | Val Acc 0.5000 | Time(s) 0.0181\n",
            "Epoch 00023 | Train Loss 0.8084 | Train Acc 0.6667 | Val Loss 1.1814 | Val Acc 0.5000 | Time(s) 0.0180\n",
            "Epoch 00024 | Train Loss 0.8301 | Train Acc 0.6917 | Val Loss 1.1870 | Val Acc 0.5125 | Time(s) 0.0180\n",
            "Epoch 00025 | Train Loss 0.7569 | Train Acc 0.6833 | Val Loss 1.2045 | Val Acc 0.4875 | Time(s) 0.0179\n",
            "Epoch 00026 | Train Loss 0.8294 | Train Acc 0.6583 | Val Loss 1.2396 | Val Acc 0.4625 | Time(s) 0.0179\n",
            "Epoch 00027 | Train Loss 0.7263 | Train Acc 0.7167 | Val Loss 1.2779 | Val Acc 0.4625 | Time(s) 0.0178\n",
            "Epoch 00028 | Train Loss 0.7244 | Train Acc 0.6667 | Val Loss 1.3029 | Val Acc 0.4625 | Time(s) 0.0178\n",
            "Epoch 00029 | Train Loss 0.7248 | Train Acc 0.6750 | Val Loss 1.3065 | Val Acc 0.4625 | Time(s) 0.0177\n",
            "Epoch 00030 | Train Loss 0.7434 | Train Acc 0.7583 | Val Loss 1.2929 | Val Acc 0.4625 | Time(s) 0.0180\n",
            "Epoch 00031 | Train Loss 0.6896 | Train Acc 0.7167 | Val Loss 1.2957 | Val Acc 0.5000 | Time(s) 0.0182\n",
            "Epoch 00032 | Train Loss 0.6435 | Train Acc 0.7417 | Val Loss 1.3061 | Val Acc 0.4875 | Time(s) 0.0182\n",
            "Epoch 00033 | Train Loss 0.7229 | Train Acc 0.7083 | Val Loss 1.3256 | Val Acc 0.5000 | Time(s) 0.0182\n",
            "Epoch 00034 | Train Loss 0.6763 | Train Acc 0.7667 | Val Loss 1.3512 | Val Acc 0.4875 | Time(s) 0.0182\n",
            "Epoch 00035 | Train Loss 0.6954 | Train Acc 0.7000 | Val Loss 1.3814 | Val Acc 0.4625 | Time(s) 0.0182\n",
            "Epoch 00036 | Train Loss 0.6518 | Train Acc 0.7417 | Val Loss 1.4123 | Val Acc 0.4750 | Time(s) 0.0182\n",
            "Epoch 00037 | Train Loss 0.6500 | Train Acc 0.7083 | Val Loss 1.4358 | Val Acc 0.4750 | Time(s) 0.0182\n",
            "Epoch 00038 | Train Loss 0.6317 | Train Acc 0.7417 | Val Loss 1.4649 | Val Acc 0.4750 | Time(s) 0.0183\n",
            "Epoch 00039 | Train Loss 0.6301 | Train Acc 0.7583 | Val Loss 1.4865 | Val Acc 0.4875 | Time(s) 0.0182\n",
            "Epoch 00040 | Train Loss 0.6251 | Train Acc 0.7333 | Val Loss 1.4986 | Val Acc 0.4750 | Time(s) 0.0183\n",
            "Epoch 00041 | Train Loss 0.6143 | Train Acc 0.7417 | Val Loss 1.5062 | Val Acc 0.4750 | Time(s) 0.0183\n",
            "Epoch 00042 | Train Loss 0.6267 | Train Acc 0.7417 | Val Loss 1.5046 | Val Acc 0.4750 | Time(s) 0.0183\n",
            "Epoch 00043 | Train Loss 0.6400 | Train Acc 0.7333 | Val Loss 1.5056 | Val Acc 0.4750 | Time(s) 0.0183\n",
            "Epoch 00044 | Train Loss 0.5744 | Train Acc 0.7500 | Val Loss 1.5189 | Val Acc 0.4625 | Time(s) 0.0183\n",
            "Epoch 00045 | Train Loss 0.5826 | Train Acc 0.7500 | Val Loss 1.5404 | Val Acc 0.4500 | Time(s) 0.0182\n",
            "Epoch 00046 | Train Loss 0.6443 | Train Acc 0.7750 | Val Loss 1.5349 | Val Acc 0.4625 | Time(s) 0.0182\n",
            "Epoch 00047 | Train Loss 0.6174 | Train Acc 0.7333 | Val Loss 1.5202 | Val Acc 0.4750 | Time(s) 0.0182\n",
            "Epoch 00048 | Train Loss 0.5848 | Train Acc 0.7333 | Val Loss 1.5129 | Val Acc 0.4750 | Time(s) 0.0182\n",
            "Epoch 00049 | Train Loss 0.5185 | Train Acc 0.7500 | Val Loss 1.5117 | Val Acc 0.4625 | Time(s) 0.0183\n",
            "Epoch 00050 | Train Loss 0.5315 | Train Acc 0.7917 | Val Loss 1.5218 | Val Acc 0.4750 | Time(s) 0.0185\n",
            "Epoch 00051 | Train Loss 0.5685 | Train Acc 0.7667 | Val Loss 1.5498 | Val Acc 0.4750 | Time(s) 0.0185\n",
            "Epoch 00052 | Train Loss 0.5682 | Train Acc 0.7667 | Val Loss 1.5932 | Val Acc 0.4875 | Time(s) 0.0185\n",
            "Epoch 00053 | Train Loss 0.5822 | Train Acc 0.7333 | Val Loss 1.6359 | Val Acc 0.4875 | Time(s) 0.0185\n",
            "Epoch 00054 | Train Loss 0.5393 | Train Acc 0.7333 | Val Loss 1.6787 | Val Acc 0.4625 | Time(s) 0.0185\n",
            "Epoch 00055 | Train Loss 0.5798 | Train Acc 0.7583 | Val Loss 1.7114 | Val Acc 0.4750 | Time(s) 0.0185\n",
            "Epoch 00056 | Train Loss 0.5130 | Train Acc 0.7500 | Val Loss 1.7362 | Val Acc 0.4875 | Time(s) 0.0184\n",
            "Epoch 00057 | Train Loss 0.5046 | Train Acc 0.8000 | Val Loss 1.7551 | Val Acc 0.4875 | Time(s) 0.0184\n",
            "Epoch 00058 | Train Loss 0.4933 | Train Acc 0.8000 | Val Loss 1.7704 | Val Acc 0.4750 | Time(s) 0.0184\n",
            "Epoch 00059 | Train Loss 0.5652 | Train Acc 0.7667 | Val Loss 1.7655 | Val Acc 0.4500 | Time(s) 0.0183\n",
            "Epoch 00060 | Train Loss 0.5073 | Train Acc 0.7667 | Val Loss 1.7641 | Val Acc 0.4625 | Time(s) 0.0184\n",
            "Epoch 00061 | Train Loss 0.6028 | Train Acc 0.7583 | Val Loss 1.7709 | Val Acc 0.4625 | Time(s) 0.0184\n",
            "Epoch 00062 | Train Loss 0.5165 | Train Acc 0.7500 | Val Loss 1.7785 | Val Acc 0.4500 | Time(s) 0.0184\n",
            "Epoch 00063 | Train Loss 0.5381 | Train Acc 0.7333 | Val Loss 1.7900 | Val Acc 0.4500 | Time(s) 0.0184\n",
            "Epoch 00064 | Train Loss 0.5523 | Train Acc 0.7667 | Val Loss 1.7968 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "Epoch 00065 | Train Loss 0.4768 | Train Acc 0.7917 | Val Loss 1.8148 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "Epoch 00066 | Train Loss 0.4901 | Train Acc 0.7917 | Val Loss 1.8161 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "Epoch 00067 | Train Loss 0.4749 | Train Acc 0.7917 | Val Loss 1.8268 | Val Acc 0.4625 | Time(s) 0.0185\n",
            "Epoch 00068 | Train Loss 0.5548 | Train Acc 0.7667 | Val Loss 1.8302 | Val Acc 0.4625 | Time(s) 0.0185\n",
            "Epoch 00069 | Train Loss 0.4748 | Train Acc 0.8167 | Val Loss 1.8293 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "Epoch 00070 | Train Loss 0.4708 | Train Acc 0.7667 | Val Loss 1.8253 | Val Acc 0.4625 | Time(s) 0.0185\n",
            "Epoch 00071 | Train Loss 0.4977 | Train Acc 0.7833 | Val Loss 1.8197 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "Epoch 00072 | Train Loss 0.4481 | Train Acc 0.8250 | Val Loss 1.8188 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "Epoch 00073 | Train Loss 0.5361 | Train Acc 0.7667 | Val Loss 1.8248 | Val Acc 0.4625 | Time(s) 0.0185\n",
            "Epoch 00074 | Train Loss 0.4946 | Train Acc 0.7667 | Val Loss 1.8366 | Val Acc 0.4625 | Time(s) 0.0185\n",
            "Epoch 00075 | Train Loss 0.4587 | Train Acc 0.8167 | Val Loss 1.8559 | Val Acc 0.4625 | Time(s) 0.0185\n",
            "Epoch 00076 | Train Loss 0.4593 | Train Acc 0.7667 | Val Loss 1.8842 | Val Acc 0.4750 | Time(s) 0.0185\n",
            "Epoch 00077 | Train Loss 0.5048 | Train Acc 0.7583 | Val Loss 1.9047 | Val Acc 0.4750 | Time(s) 0.0185\n",
            "Epoch 00078 | Train Loss 0.4801 | Train Acc 0.8000 | Val Loss 1.9103 | Val Acc 0.4625 | Time(s) 0.0185\n",
            "Epoch 00079 | Train Loss 0.5027 | Train Acc 0.8083 | Val Loss 1.9043 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "Epoch 00080 | Train Loss 0.4614 | Train Acc 0.8417 | Val Loss 1.8917 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "Epoch 00081 | Train Loss 0.4405 | Train Acc 0.8417 | Val Loss 1.8823 | Val Acc 0.4625 | Time(s) 0.0185\n",
            "Epoch 00082 | Train Loss 0.4822 | Train Acc 0.7917 | Val Loss 1.8811 | Val Acc 0.4875 | Time(s) 0.0185\n",
            "Epoch 00083 | Train Loss 0.4752 | Train Acc 0.7917 | Val Loss 1.8792 | Val Acc 0.4875 | Time(s) 0.0186\n",
            "Epoch 00084 | Train Loss 0.4322 | Train Acc 0.8250 | Val Loss 1.8785 | Val Acc 0.4625 | Time(s) 0.0187\n",
            "Epoch 00085 | Train Loss 0.4056 | Train Acc 0.8417 | Val Loss 1.8847 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "Epoch 00086 | Train Loss 0.4526 | Train Acc 0.8417 | Val Loss 1.8972 | Val Acc 0.4625 | Time(s) 0.0187\n",
            "Epoch 00087 | Train Loss 0.4278 | Train Acc 0.7917 | Val Loss 1.9191 | Val Acc 0.4750 | Time(s) 0.0186\n",
            "Epoch 00088 | Train Loss 0.4520 | Train Acc 0.8333 | Val Loss 1.9363 | Val Acc 0.4750 | Time(s) 0.0186\n",
            "Epoch 00089 | Train Loss 0.4901 | Train Acc 0.8083 | Val Loss 1.9668 | Val Acc 0.4750 | Time(s) 0.0187\n",
            "Epoch 00090 | Train Loss 0.4262 | Train Acc 0.8333 | Val Loss 1.9948 | Val Acc 0.4750 | Time(s) 0.0186\n",
            "Epoch 00091 | Train Loss 0.4704 | Train Acc 0.7833 | Val Loss 2.0201 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "Epoch 00092 | Train Loss 0.4794 | Train Acc 0.7667 | Val Loss 2.0199 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "Epoch 00093 | Train Loss 0.4309 | Train Acc 0.8417 | Val Loss 2.0095 | Val Acc 0.4750 | Time(s) 0.0186\n",
            "Epoch 00094 | Train Loss 0.4235 | Train Acc 0.8250 | Val Loss 1.9952 | Val Acc 0.4875 | Time(s) 0.0186\n",
            "Epoch 00095 | Train Loss 0.4574 | Train Acc 0.7917 | Val Loss 2.0054 | Val Acc 0.4875 | Time(s) 0.0186\n",
            "Epoch 00096 | Train Loss 0.4252 | Train Acc 0.8583 | Val Loss 2.0167 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "Epoch 00097 | Train Loss 0.4811 | Train Acc 0.8250 | Val Loss 2.0355 | Val Acc 0.4750 | Time(s) 0.0186\n",
            "Epoch 00098 | Train Loss 0.4735 | Train Acc 0.8167 | Val Loss 2.0618 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "Epoch 00099 | Train Loss 0.4214 | Train Acc 0.7917 | Val Loss 2.0836 | Val Acc 0.4625 | Time(s) 0.0185\n",
            "Epoch 00100 | Train Loss 0.4095 | Train Acc 0.8750 | Val Loss 2.0963 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "Epoch 00101 | Train Loss 0.4352 | Train Acc 0.8083 | Val Loss 2.1179 | Val Acc 0.4625 | Time(s) 0.0185\n",
            "Epoch 00102 | Train Loss 0.4242 | Train Acc 0.8167 | Val Loss 2.1270 | Val Acc 0.4625 | Time(s) 0.0185\n",
            "Epoch 00103 | Train Loss 0.3602 | Train Acc 0.8583 | Val Loss 2.1287 | Val Acc 0.4625 | Time(s) 0.0185\n",
            "Epoch 00104 | Train Loss 0.4333 | Train Acc 0.8333 | Val Loss 2.1203 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "Epoch 00105 | Train Loss 0.4198 | Train Acc 0.8167 | Val Loss 2.1085 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "Epoch 00106 | Train Loss 0.3783 | Train Acc 0.8500 | Val Loss 2.0981 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "Epoch 00107 | Train Loss 0.3973 | Train Acc 0.8167 | Val Loss 2.0850 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "Epoch 00108 | Train Loss 0.3948 | Train Acc 0.8417 | Val Loss 2.0792 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "Epoch 00109 | Train Loss 0.4079 | Train Acc 0.8083 | Val Loss 2.0737 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "Epoch 00110 | Train Loss 0.4285 | Train Acc 0.8250 | Val Loss 2.0707 | Val Acc 0.4375 | Time(s) 0.0187\n",
            "Epoch 00111 | Train Loss 0.3872 | Train Acc 0.8583 | Val Loss 2.0817 | Val Acc 0.4250 | Time(s) 0.0187\n",
            "Epoch 00112 | Train Loss 0.4293 | Train Acc 0.8250 | Val Loss 2.1076 | Val Acc 0.4500 | Time(s) 0.0187\n",
            "Epoch 00113 | Train Loss 0.4483 | Train Acc 0.8500 | Val Loss 2.1488 | Val Acc 0.4625 | Time(s) 0.0187\n",
            "Epoch 00114 | Train Loss 0.4151 | Train Acc 0.8667 | Val Loss 2.1711 | Val Acc 0.4625 | Time(s) 0.0187\n",
            "\n",
            "Epoch 00115 | Train Loss 0.4133 | Train Acc 0.8333 | Val Loss 2.1804 | Val Acc 0.4625 | Time(s) 0.0187\n",
            "\n",
            "Epoch 00116 | Train Loss 0.4039 | Train Acc 0.8167 | Val Loss 2.1791 | Val Acc 0.4625 | Time(s) 0.0187\n",
            "\n",
            "Epoch 00117 | Train Loss 0.3847 | Train Acc 0.8333 | Val Loss 2.1773 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00118 | Train Loss 0.4105 | Train Acc 0.8250 | Val Loss 2.1796 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00119 | Train Loss 0.5116 | Train Acc 0.7917 | Val Loss 2.1563 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00120 | Train Loss 0.3929 | Train Acc 0.8417 | Val Loss 2.1353 | Val Acc 0.4375 | Time(s) 0.0187\n",
            "\n",
            "Epoch 00121 | Train Loss 0.4511 | Train Acc 0.8250 | Val Loss 2.1245 | Val Acc 0.4500 | Time(s) 0.0187\n",
            "\n",
            "Epoch 00122 | Train Loss 0.3671 | Train Acc 0.8333 | Val Loss 2.1216 | Val Acc 0.4500 | Time(s) 0.0187\n",
            "\n",
            "Epoch 00123 | Train Loss 0.4296 | Train Acc 0.8167 | Val Loss 2.1333 | Val Acc 0.4500 | Time(s) 0.0187\n",
            "\n",
            "Epoch 00124 | Train Loss 0.4240 | Train Acc 0.8000 | Val Loss 2.1570 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00125 | Train Loss 0.3653 | Train Acc 0.8833 | Val Loss 2.1778 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00126 | Train Loss 0.3717 | Train Acc 0.8417 | Val Loss 2.2010 | Val Acc 0.4750 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00127 | Train Loss 0.3798 | Train Acc 0.8417 | Val Loss 2.2152 | Val Acc 0.4750 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00128 | Train Loss 0.3503 | Train Acc 0.8750 | Val Loss 2.2254 | Val Acc 0.4750 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00129 | Train Loss 0.4991 | Train Acc 0.7667 | Val Loss 2.2175 | Val Acc 0.4750 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00130 | Train Loss 0.4120 | Train Acc 0.8333 | Val Loss 2.1989 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00131 | Train Loss 0.3801 | Train Acc 0.8167 | Val Loss 2.1959 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00132 | Train Loss 0.4015 | Train Acc 0.8333 | Val Loss 2.1884 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00133 | Train Loss 0.3940 | Train Acc 0.8333 | Val Loss 2.1783 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00134 | Train Loss 0.3359 | Train Acc 0.8500 | Val Loss 2.1689 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00135 | Train Loss 0.3674 | Train Acc 0.8583 | Val Loss 2.1668 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00136 | Train Loss 0.3796 | Train Acc 0.8750 | Val Loss 2.1687 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00137 | Train Loss 0.4703 | Train Acc 0.8167 | Val Loss 2.1748 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00138 | Train Loss 0.3513 | Train Acc 0.8583 | Val Loss 2.1900 | Val Acc 0.4125 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00139 | Train Loss 0.3802 | Train Acc 0.8750 | Val Loss 2.2118 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00140 | Train Loss 0.3537 | Train Acc 0.9083 | Val Loss 2.2268 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00141 | Train Loss 0.3734 | Train Acc 0.8667 | Val Loss 2.2494 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00142 | Train Loss 0.3492 | Train Acc 0.8417 | Val Loss 2.2737 | Val Acc 0.4250 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00143 | Train Loss 0.3885 | Train Acc 0.8667 | Val Loss 2.3114 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00144 | Train Loss 0.3862 | Train Acc 0.8333 | Val Loss 2.3390 | Val Acc 0.4250 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00145 | Train Loss 0.3942 | Train Acc 0.8333 | Val Loss 2.3482 | Val Acc 0.4250 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00146 | Train Loss 0.3832 | Train Acc 0.8667 | Val Loss 2.3404 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00147 | Train Loss 0.4160 | Train Acc 0.8417 | Val Loss 2.3217 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00148 | Train Loss 0.4179 | Train Acc 0.8167 | Val Loss 2.2984 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00149 | Train Loss 0.3523 | Train Acc 0.8500 | Val Loss 2.2841 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00150 | Train Loss 0.3279 | Train Acc 0.8833 | Val Loss 2.2778 | Val Acc 0.4500 | Time(s) 0.0187\n",
            "\n",
            "Epoch 00151 | Train Loss 0.5053 | Train Acc 0.7833 | Val Loss 2.2759 | Val Acc 0.4500 | Time(s) 0.0187\n",
            "\n",
            "Epoch 00152 | Train Loss 0.3591 | Train Acc 0.8250 | Val Loss 2.2836 | Val Acc 0.4375 | Time(s) 0.0187\n",
            "\n",
            "Epoch 00153 | Train Loss 0.3230 | Train Acc 0.9083 | Val Loss 2.2939 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00154 | Train Loss 0.3037 | Train Acc 0.9167 | Val Loss 2.3060 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00155 | Train Loss 0.4281 | Train Acc 0.8167 | Val Loss 2.3111 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00156 | Train Loss 0.3752 | Train Acc 0.8667 | Val Loss 2.3138 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00157 | Train Loss 0.3660 | Train Acc 0.8583 | Val Loss 2.3166 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00158 | Train Loss 0.3734 | Train Acc 0.8667 | Val Loss 2.3238 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00159 | Train Loss 0.3580 | Train Acc 0.8833 | Val Loss 2.3378 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00160 | Train Loss 0.3509 | Train Acc 0.8500 | Val Loss 2.3395 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00161 | Train Loss 0.3384 | Train Acc 0.8833 | Val Loss 2.3410 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00162 | Train Loss 0.3639 | Train Acc 0.8750 | Val Loss 2.3492 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00163 | Train Loss 0.3425 | Train Acc 0.8667 | Val Loss 2.3552 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00164 | Train Loss 0.3397 | Train Acc 0.8583 | Val Loss 2.3752 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00165 | Train Loss 0.3098 | Train Acc 0.8917 | Val Loss 2.3937 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00166 | Train Loss 0.3067 | Train Acc 0.9000 | Val Loss 2.4129 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00167 | Train Loss 0.3490 | Train Acc 0.8750 | Val Loss 2.4315 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00168 | Train Loss 0.3322 | Train Acc 0.8750 | Val Loss 2.4401 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00169 | Train Loss 0.3617 | Train Acc 0.8167 | Val Loss 2.4505 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00170 | Train Loss 0.3498 | Train Acc 0.8750 | Val Loss 2.4627 | Val Acc 0.4625 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00171 | Train Loss 0.4022 | Train Acc 0.8667 | Val Loss 2.4662 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00172 | Train Loss 0.3682 | Train Acc 0.8500 | Val Loss 2.4725 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00173 | Train Loss 0.3344 | Train Acc 0.8917 | Val Loss 2.4856 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00174 | Train Loss 0.3131 | Train Acc 0.8917 | Val Loss 2.4966 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00175 | Train Loss 0.3333 | Train Acc 0.8500 | Val Loss 2.5061 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00176 | Train Loss 0.4543 | Train Acc 0.8083 | Val Loss 2.5070 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00177 | Train Loss 0.3715 | Train Acc 0.8417 | Val Loss 2.5146 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00178 | Train Loss 0.3370 | Train Acc 0.8417 | Val Loss 2.5187 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00179 | Train Loss 0.3632 | Train Acc 0.8667 | Val Loss 2.5090 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00180 | Train Loss 0.3509 | Train Acc 0.8667 | Val Loss 2.4966 | Val Acc 0.4250 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00181 | Train Loss 0.3652 | Train Acc 0.8333 | Val Loss 2.4949 | Val Acc 0.4250 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00182 | Train Loss 0.3199 | Train Acc 0.9000 | Val Loss 2.4855 | Val Acc 0.4250 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00183 | Train Loss 0.3485 | Train Acc 0.8750 | Val Loss 2.4734 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00184 | Train Loss 0.3573 | Train Acc 0.8583 | Val Loss 2.4623 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00185 | Train Loss 0.3515 | Train Acc 0.8500 | Val Loss 2.4563 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00186 | Train Loss 0.3866 | Train Acc 0.8417 | Val Loss 2.4484 | Val Acc 0.4625 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00187 | Train Loss 0.3449 | Train Acc 0.8667 | Val Loss 2.4404 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00188 | Train Loss 0.3738 | Train Acc 0.8417 | Val Loss 2.4514 | Val Acc 0.4625 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00189 | Train Loss 0.3307 | Train Acc 0.8833 | Val Loss 2.4651 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00190 | Train Loss 0.3635 | Train Acc 0.8500 | Val Loss 2.4889 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00191 | Train Loss 0.3888 | Train Acc 0.8833 | Val Loss 2.5049 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00192 | Train Loss 0.3881 | Train Acc 0.8750 | Val Loss 2.5094 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00193 | Train Loss 0.3589 | Train Acc 0.8250 | Val Loss 2.5110 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00194 | Train Loss 0.3403 | Train Acc 0.8667 | Val Loss 2.5131 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00195 | Train Loss 0.3230 | Train Acc 0.8417 | Val Loss 2.5198 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00196 | Train Loss 0.3322 | Train Acc 0.8583 | Val Loss 2.5299 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00197 | Train Loss 0.4175 | Train Acc 0.8083 | Val Loss 2.5256 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00198 | Train Loss 0.3112 | Train Acc 0.9000 | Val Loss 2.5188 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00199 | Train Loss 0.3676 | Train Acc 0.8667 | Val Loss 2.5028 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00200 | Train Loss 0.3465 | Train Acc 0.8583 | Val Loss 2.4929 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00201 | Train Loss 0.3430 | Train Acc 0.8917 | Val Loss 2.4875 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00202 | Train Loss 0.3675 | Train Acc 0.8417 | Val Loss 2.4844 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00203 | Train Loss 0.3142 | Train Acc 0.8917 | Val Loss 2.4832 | Val Acc 0.4250 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00204 | Train Loss 0.3988 | Train Acc 0.8167 | Val Loss 2.4863 | Val Acc 0.4375 | Time(s) 0.0187\n",
            "\n",
            "Epoch 00205 | Train Loss 0.3456 | Train Acc 0.8583 | Val Loss 2.4985 | Val Acc 0.4375 | Time(s) 0.0187\n",
            "\n",
            "Epoch 00206 | Train Loss 0.3345 | Train Acc 0.8667 | Val Loss 2.5167 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00207 | Train Loss 0.3544 | Train Acc 0.8833 | Val Loss 2.5396 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00208 | Train Loss 0.2972 | Train Acc 0.8833 | Val Loss 2.5630 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00209 | Train Loss 0.3458 | Train Acc 0.8417 | Val Loss 2.5755 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00210 | Train Loss 0.3318 | Train Acc 0.8667 | Val Loss 2.5797 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00211 | Train Loss 0.3164 | Train Acc 0.8750 | Val Loss 2.5793 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00212 | Train Loss 0.3112 | Train Acc 0.8750 | Val Loss 2.5749 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00213 | Train Loss 0.3902 | Train Acc 0.8667 | Val Loss 2.5655 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00214 | Train Loss 0.3121 | Train Acc 0.8583 | Val Loss 2.5610 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00215 | Train Loss 0.3574 | Train Acc 0.9000 | Val Loss 2.5476 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00216 | Train Loss 0.3402 | Train Acc 0.8917 | Val Loss 2.5440 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00217 | Train Loss 0.3736 | Train Acc 0.8833 | Val Loss 2.5374 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00218 | Train Loss 0.3545 | Train Acc 0.8750 | Val Loss 2.5307 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00219 | Train Loss 0.3442 | Train Acc 0.8667 | Val Loss 2.5239 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00220 | Train Loss 0.3633 | Train Acc 0.8917 | Val Loss 2.5173 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00221 | Train Loss 0.3333 | Train Acc 0.8750 | Val Loss 2.5143 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00222 | Train Loss 0.3753 | Train Acc 0.8500 | Val Loss 2.5193 | Val Acc 0.4500 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00223 | Train Loss 0.3572 | Train Acc 0.8333 | Val Loss 2.5251 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00224 | Train Loss 0.3189 | Train Acc 0.8833 | Val Loss 2.5360 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00225 | Train Loss 0.3133 | Train Acc 0.9083 | Val Loss 2.5452 | Val Acc 0.4375 | Time(s) 0.0186\n",
            "\n",
            "Epoch 00226 | Train Loss 0.3329 | Train Acc 0.8667 | Val Loss 2.5592 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00227 | Train Loss 0.3930 | Train Acc 0.8667 | Val Loss 2.5706 | Val Acc 0.4250 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00228 | Train Loss 0.3226 | Train Acc 0.8667 | Val Loss 2.5892 | Val Acc 0.4250 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00229 | Train Loss 0.2996 | Train Acc 0.8750 | Val Loss 2.6026 | Val Acc 0.4250 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00230 | Train Loss 0.2978 | Train Acc 0.8750 | Val Loss 2.6093 | Val Acc 0.4250 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00231 | Train Loss 0.2865 | Train Acc 0.9000 | Val Loss 2.6178 | Val Acc 0.4250 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00232 | Train Loss 0.3706 | Train Acc 0.8583 | Val Loss 2.6277 | Val Acc 0.4250 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00233 | Train Loss 0.4059 | Train Acc 0.8250 | Val Loss 2.6466 | Val Acc 0.4250 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00234 | Train Loss 0.3564 | Train Acc 0.8417 | Val Loss 2.6534 | Val Acc 0.4250 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00235 | Train Loss 0.3042 | Train Acc 0.9000 | Val Loss 2.6629 | Val Acc 0.4250 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00236 | Train Loss 0.3185 | Train Acc 0.8833 | Val Loss 2.6717 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00237 | Train Loss 0.3762 | Train Acc 0.8417 | Val Loss 2.6706 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00238 | Train Loss 0.3219 | Train Acc 0.8583 | Val Loss 2.6715 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00239 | Train Loss 0.3055 | Train Acc 0.8917 | Val Loss 2.6720 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00240 | Train Loss 0.2938 | Train Acc 0.9250 | Val Loss 2.6678 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00241 | Train Loss 0.3041 | Train Acc 0.9167 | Val Loss 2.6671 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00242 | Train Loss 0.3417 | Train Acc 0.8500 | Val Loss 2.6670 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00243 | Train Loss 0.2931 | Train Acc 0.8917 | Val Loss 2.6683 | Val Acc 0.4500 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00244 | Train Loss 0.3373 | Train Acc 0.8583 | Val Loss 2.6635 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00245 | Train Loss 0.3365 | Train Acc 0.8667 | Val Loss 2.6679 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00246 | Train Loss 0.3475 | Train Acc 0.8333 | Val Loss 2.6811 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00247 | Train Loss 0.3487 | Train Acc 0.8583 | Val Loss 2.6938 | Val Acc 0.4500 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00248 | Train Loss 0.3352 | Train Acc 0.8750 | Val Loss 2.7092 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00249 | Train Loss 0.3101 | Train Acc 0.9250 | Val Loss 2.7211 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00250 | Train Loss 0.3049 | Train Acc 0.9083 | Val Loss 2.7328 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00251 | Train Loss 0.3332 | Train Acc 0.8583 | Val Loss 2.7480 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00252 | Train Loss 0.3992 | Train Acc 0.8917 | Val Loss 2.7518 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00253 | Train Loss 0.3302 | Train Acc 0.9000 | Val Loss 2.7586 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00254 | Train Loss 0.2867 | Train Acc 0.9000 | Val Loss 2.7696 | Val Acc 0.4500 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00255 | Train Loss 0.3603 | Train Acc 0.8583 | Val Loss 2.7721 | Val Acc 0.4500 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00256 | Train Loss 0.3370 | Train Acc 0.8667 | Val Loss 2.7640 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00257 | Train Loss 0.3100 | Train Acc 0.8833 | Val Loss 2.7580 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00258 | Train Loss 0.3317 | Train Acc 0.8500 | Val Loss 2.7542 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00259 | Train Loss 0.3145 | Train Acc 0.8917 | Val Loss 2.7480 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00260 | Train Loss 0.3124 | Train Acc 0.8667 | Val Loss 2.7366 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00261 | Train Loss 0.3445 | Train Acc 0.8583 | Val Loss 2.7311 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00262 | Train Loss 0.3149 | Train Acc 0.8917 | Val Loss 2.7246 | Val Acc 0.4375 | Time(s) 0.0185\n",
            "\n",
            "Epoch 00263 | Train Loss 0.2767 | Train Acc 0.8833 | Val Loss 2.7170 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00264 | Train Loss 0.3678 | Train Acc 0.8500 | Val Loss 2.7216 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00265 | Train Loss 0.3023 | Train Acc 0.8750 | Val Loss 2.7273 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00266 | Train Loss 0.2641 | Train Acc 0.8833 | Val Loss 2.7343 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00267 | Train Loss 0.3042 | Train Acc 0.8750 | Val Loss 2.7394 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00268 | Train Loss 0.3463 | Train Acc 0.8750 | Val Loss 2.7429 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00269 | Train Loss 0.2700 | Train Acc 0.9250 | Val Loss 2.7438 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00270 | Train Loss 0.2796 | Train Acc 0.8833 | Val Loss 2.7424 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00271 | Train Loss 0.3262 | Train Acc 0.8500 | Val Loss 2.7480 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00272 | Train Loss 0.3460 | Train Acc 0.8750 | Val Loss 2.7521 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00273 | Train Loss 0.3640 | Train Acc 0.8417 | Val Loss 2.7549 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00274 | Train Loss 0.3242 | Train Acc 0.8250 | Val Loss 2.7528 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00275 | Train Loss 0.3078 | Train Acc 0.9167 | Val Loss 2.7536 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00276 | Train Loss 0.3268 | Train Acc 0.8500 | Val Loss 2.7554 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00277 | Train Loss 0.3285 | Train Acc 0.8500 | Val Loss 2.7516 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00278 | Train Loss 0.3057 | Train Acc 0.8833 | Val Loss 2.7478 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00279 | Train Loss 0.2708 | Train Acc 0.9000 | Val Loss 2.7468 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00280 | Train Loss 0.3476 | Train Acc 0.8667 | Val Loss 2.7511 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00281 | Train Loss 0.2893 | Train Acc 0.9083 | Val Loss 2.7598 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00282 | Train Loss 0.3667 | Train Acc 0.8333 | Val Loss 2.7615 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00283 | Train Loss 0.3219 | Train Acc 0.9083 | Val Loss 2.7649 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00284 | Train Loss 0.3070 | Train Acc 0.8583 | Val Loss 2.7641 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00285 | Train Loss 0.3201 | Train Acc 0.8833 | Val Loss 2.7702 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00286 | Train Loss 0.3601 | Train Acc 0.8583 | Val Loss 2.7827 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00287 | Train Loss 0.3139 | Train Acc 0.8750 | Val Loss 2.7915 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00288 | Train Loss 0.2837 | Train Acc 0.9000 | Val Loss 2.7983 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00289 | Train Loss 0.2729 | Train Acc 0.8833 | Val Loss 2.8079 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00290 | Train Loss 0.3449 | Train Acc 0.8750 | Val Loss 2.8149 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00291 | Train Loss 0.2870 | Train Acc 0.8833 | Val Loss 2.8210 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00292 | Train Loss 0.3224 | Train Acc 0.8833 | Val Loss 2.8290 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00293 | Train Loss 0.3351 | Train Acc 0.8667 | Val Loss 2.8345 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00294 | Train Loss 0.2814 | Train Acc 0.9083 | Val Loss 2.8370 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00295 | Train Loss 0.3532 | Train Acc 0.8583 | Val Loss 2.8322 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00296 | Train Loss 0.2462 | Train Acc 0.9083 | Val Loss 2.8253 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00297 | Train Loss 0.3179 | Train Acc 0.8750 | Val Loss 2.8185 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00298 | Train Loss 0.3004 | Train Acc 0.8667 | Val Loss 2.8123 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00299 | Train Loss 0.3415 | Train Acc 0.8667 | Val Loss 2.8032 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00300 | Train Loss 0.3184 | Train Acc 0.8750 | Val Loss 2.7933 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00301 | Train Loss 0.2715 | Train Acc 0.9000 | Val Loss 2.7904 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00302 | Train Loss 0.2980 | Train Acc 0.8833 | Val Loss 2.7893 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00303 | Train Loss 0.2806 | Train Acc 0.8750 | Val Loss 2.7895 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00304 | Train Loss 0.3069 | Train Acc 0.8667 | Val Loss 2.7969 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00305 | Train Loss 0.3168 | Train Acc 0.9083 | Val Loss 2.8013 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00306 | Train Loss 0.3070 | Train Acc 0.8917 | Val Loss 2.8046 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00307 | Train Loss 0.3124 | Train Acc 0.8417 | Val Loss 2.8084 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00308 | Train Loss 0.3044 | Train Acc 0.8917 | Val Loss 2.8117 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00309 | Train Loss 0.3083 | Train Acc 0.8750 | Val Loss 2.8202 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00310 | Train Loss 0.2863 | Train Acc 0.9167 | Val Loss 2.8270 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00311 | Train Loss 0.2695 | Train Acc 0.9000 | Val Loss 2.8364 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00312 | Train Loss 0.2938 | Train Acc 0.9083 | Val Loss 2.8414 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00313 | Train Loss 0.2462 | Train Acc 0.9083 | Val Loss 2.8493 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00314 | Train Loss 0.2820 | Train Acc 0.9000 | Val Loss 2.8618 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00315 | Train Loss 0.3069 | Train Acc 0.8583 | Val Loss 2.8680 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00316 | Train Loss 0.3252 | Train Acc 0.8583 | Val Loss 2.8695 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00317 | Train Loss 0.2859 | Train Acc 0.8833 | Val Loss 2.8696 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00318 | Train Loss 0.3352 | Train Acc 0.8583 | Val Loss 2.8728 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00319 | Train Loss 0.3193 | Train Acc 0.8583 | Val Loss 2.8780 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00320 | Train Loss 0.3219 | Train Acc 0.8583 | Val Loss 2.8801 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00321 | Train Loss 0.2956 | Train Acc 0.8750 | Val Loss 2.8792 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00322 | Train Loss 0.2922 | Train Acc 0.9000 | Val Loss 2.8777 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00323 | Train Loss 0.3389 | Train Acc 0.8583 | Val Loss 2.8812 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00324 | Train Loss 0.2799 | Train Acc 0.8833 | Val Loss 2.8832 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00325 | Train Loss 0.2779 | Train Acc 0.9000 | Val Loss 2.8828 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00326 | Train Loss 0.3131 | Train Acc 0.8667 | Val Loss 2.8844 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00327 | Train Loss 0.3020 | Train Acc 0.8750 | Val Loss 2.8873 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00328 | Train Loss 0.2934 | Train Acc 0.8750 | Val Loss 2.8890 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00329 | Train Loss 0.3245 | Train Acc 0.8667 | Val Loss 2.8931 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00330 | Train Loss 0.3101 | Train Acc 0.8833 | Val Loss 2.9003 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00331 | Train Loss 0.3203 | Train Acc 0.9000 | Val Loss 2.9019 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00332 | Train Loss 0.2931 | Train Acc 0.8917 | Val Loss 2.9030 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00333 | Train Loss 0.2794 | Train Acc 0.8917 | Val Loss 2.9061 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00334 | Train Loss 0.2495 | Train Acc 0.9083 | Val Loss 2.9106 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00335 | Train Loss 0.3268 | Train Acc 0.8333 | Val Loss 2.9152 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00336 | Train Loss 0.2531 | Train Acc 0.8833 | Val Loss 2.9187 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00337 | Train Loss 0.2839 | Train Acc 0.9083 | Val Loss 2.9255 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00338 | Train Loss 0.3037 | Train Acc 0.8750 | Val Loss 2.9322 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00339 | Train Loss 0.3141 | Train Acc 0.9250 | Val Loss 2.9327 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00340 | Train Loss 0.3171 | Train Acc 0.8750 | Val Loss 2.9279 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00341 | Train Loss 0.2463 | Train Acc 0.9250 | Val Loss 2.9265 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00342 | Train Loss 0.2743 | Train Acc 0.9083 | Val Loss 2.9234 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00343 | Train Loss 0.2736 | Train Acc 0.8750 | Val Loss 2.9180 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00344 | Train Loss 0.3266 | Train Acc 0.8583 | Val Loss 2.9055 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00345 | Train Loss 0.3210 | Train Acc 0.8500 | Val Loss 2.8922 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00346 | Train Loss 0.3092 | Train Acc 0.8750 | Val Loss 2.8778 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00347 | Train Loss 0.3184 | Train Acc 0.8500 | Val Loss 2.8658 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00348 | Train Loss 0.2613 | Train Acc 0.9250 | Val Loss 2.8599 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00349 | Train Loss 0.2621 | Train Acc 0.9250 | Val Loss 2.8558 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00350 | Train Loss 0.2964 | Train Acc 0.8750 | Val Loss 2.8579 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00351 | Train Loss 0.2771 | Train Acc 0.8667 | Val Loss 2.8624 | Val Acc 0.4250 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00352 | Train Loss 0.3891 | Train Acc 0.8750 | Val Loss 2.8725 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00353 | Train Loss 0.3118 | Train Acc 0.8833 | Val Loss 2.8816 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00354 | Train Loss 0.2936 | Train Acc 0.8750 | Val Loss 2.8871 | Val Acc 0.4500 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00355 | Train Loss 0.2649 | Train Acc 0.9000 | Val Loss 2.8953 | Val Acc 0.4500 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00356 | Train Loss 0.3131 | Train Acc 0.8500 | Val Loss 2.9046 | Val Acc 0.4500 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00357 | Train Loss 0.3016 | Train Acc 0.8833 | Val Loss 2.9112 | Val Acc 0.4500 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00358 | Train Loss 0.2785 | Train Acc 0.8917 | Val Loss 2.9206 | Val Acc 0.4500 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00359 | Train Loss 0.2991 | Train Acc 0.9000 | Val Loss 2.9271 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00360 | Train Loss 0.3072 | Train Acc 0.8917 | Val Loss 2.9309 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00361 | Train Loss 0.3240 | Train Acc 0.8583 | Val Loss 2.9482 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00362 | Train Loss 0.2726 | Train Acc 0.9000 | Val Loss 2.9650 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00363 | Train Loss 0.2738 | Train Acc 0.9333 | Val Loss 2.9770 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00364 | Train Loss 0.2785 | Train Acc 0.8833 | Val Loss 2.9882 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00365 | Train Loss 0.3212 | Train Acc 0.8917 | Val Loss 2.9923 | Val Acc 0.4375 | Time(s) 0.0184\n",
            "\n",
            "Epoch 00366 | Train Loss 0.2777 | Train Acc 0.8833 | Val Loss 2.9989 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00367 | Train Loss 0.3040 | Train Acc 0.8833 | Val Loss 3.0007 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00368 | Train Loss 0.2675 | Train Acc 0.8833 | Val Loss 2.9987 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00369 | Train Loss 0.3761 | Train Acc 0.8500 | Val Loss 2.9823 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00370 | Train Loss 0.3183 | Train Acc 0.8917 | Val Loss 2.9588 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00371 | Train Loss 0.3400 | Train Acc 0.8583 | Val Loss 2.9365 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00372 | Train Loss 0.2741 | Train Acc 0.9167 | Val Loss 2.9215 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00373 | Train Loss 0.2889 | Train Acc 0.8833 | Val Loss 2.9119 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00374 | Train Loss 0.2652 | Train Acc 0.9250 | Val Loss 2.9044 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00375 | Train Loss 0.3006 | Train Acc 0.8833 | Val Loss 2.8967 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00376 | Train Loss 0.2781 | Train Acc 0.9000 | Val Loss 2.8940 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00377 | Train Loss 0.3531 | Train Acc 0.8250 | Val Loss 2.8961 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00378 | Train Loss 0.2434 | Train Acc 0.9333 | Val Loss 2.8984 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00379 | Train Loss 0.2864 | Train Acc 0.9083 | Val Loss 2.8999 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00380 | Train Loss 0.2789 | Train Acc 0.9250 | Val Loss 2.9036 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00381 | Train Loss 0.3389 | Train Acc 0.8500 | Val Loss 2.9116 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00382 | Train Loss 0.3123 | Train Acc 0.8500 | Val Loss 2.9201 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00383 | Train Loss 0.2868 | Train Acc 0.8833 | Val Loss 2.9294 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00384 | Train Loss 0.3490 | Train Acc 0.8500 | Val Loss 2.9334 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00385 | Train Loss 0.2175 | Train Acc 0.9250 | Val Loss 2.9379 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00386 | Train Loss 0.2693 | Train Acc 0.9250 | Val Loss 2.9452 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00387 | Train Loss 0.3255 | Train Acc 0.9000 | Val Loss 2.9517 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00388 | Train Loss 0.3011 | Train Acc 0.8833 | Val Loss 2.9571 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00389 | Train Loss 0.2914 | Train Acc 0.9000 | Val Loss 2.9609 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00390 | Train Loss 0.2415 | Train Acc 0.9500 | Val Loss 2.9663 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00391 | Train Loss 0.2667 | Train Acc 0.9083 | Val Loss 2.9688 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00392 | Train Loss 0.3269 | Train Acc 0.8750 | Val Loss 2.9726 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00393 | Train Loss 0.2813 | Train Acc 0.9083 | Val Loss 2.9735 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00394 | Train Loss 0.3347 | Train Acc 0.8667 | Val Loss 2.9730 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00395 | Train Loss 0.2876 | Train Acc 0.9167 | Val Loss 2.9717 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00396 | Train Loss 0.3004 | Train Acc 0.9083 | Val Loss 2.9695 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00397 | Train Loss 0.3189 | Train Acc 0.8750 | Val Loss 2.9718 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00398 | Train Loss 0.2656 | Train Acc 0.9083 | Val Loss 2.9746 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00399 | Train Loss 0.2506 | Train Acc 0.8833 | Val Loss 2.9786 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00400 | Train Loss 0.2850 | Train Acc 0.8833 | Val Loss 2.9792 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00401 | Train Loss 0.2682 | Train Acc 0.9167 | Val Loss 2.9790 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00402 | Train Loss 0.2775 | Train Acc 0.9000 | Val Loss 2.9767 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00403 | Train Loss 0.2906 | Train Acc 0.8667 | Val Loss 2.9772 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00404 | Train Loss 0.3295 | Train Acc 0.8917 | Val Loss 2.9802 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00405 | Train Loss 0.2441 | Train Acc 0.9000 | Val Loss 2.9829 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00406 | Train Loss 0.3364 | Train Acc 0.8583 | Val Loss 2.9856 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00407 | Train Loss 0.3252 | Train Acc 0.8833 | Val Loss 2.9873 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00408 | Train Loss 0.3454 | Train Acc 0.8750 | Val Loss 2.9834 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00409 | Train Loss 0.3070 | Train Acc 0.8917 | Val Loss 2.9827 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00410 | Train Loss 0.2499 | Train Acc 0.8917 | Val Loss 2.9821 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00411 | Train Loss 0.2528 | Train Acc 0.8917 | Val Loss 2.9812 | Val Acc 0.4125 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00412 | Train Loss 0.2864 | Train Acc 0.9000 | Val Loss 2.9819 | Val Acc 0.4125 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00413 | Train Loss 0.2652 | Train Acc 0.9167 | Val Loss 2.9841 | Val Acc 0.4125 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00414 | Train Loss 0.2584 | Train Acc 0.9167 | Val Loss 2.9886 | Val Acc 0.4125 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00415 | Train Loss 0.3228 | Train Acc 0.8833 | Val Loss 2.9900 | Val Acc 0.4125 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00416 | Train Loss 0.3258 | Train Acc 0.8750 | Val Loss 2.9881 | Val Acc 0.4125 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00417 | Train Loss 0.2600 | Train Acc 0.8750 | Val Loss 2.9853 | Val Acc 0.4125 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00418 | Train Loss 0.2864 | Train Acc 0.9083 | Val Loss 2.9804 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00419 | Train Loss 0.3146 | Train Acc 0.8750 | Val Loss 2.9796 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00420 | Train Loss 0.2917 | Train Acc 0.8833 | Val Loss 2.9832 | Val Acc 0.4125 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00421 | Train Loss 0.2690 | Train Acc 0.9167 | Val Loss 2.9886 | Val Acc 0.4125 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00422 | Train Loss 0.2466 | Train Acc 0.9083 | Val Loss 2.9948 | Val Acc 0.4125 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00423 | Train Loss 0.2704 | Train Acc 0.8750 | Val Loss 3.0052 | Val Acc 0.4125 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00424 | Train Loss 0.2334 | Train Acc 0.9167 | Val Loss 3.0173 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00425 | Train Loss 0.2554 | Train Acc 0.8917 | Val Loss 3.0315 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00426 | Train Loss 0.2876 | Train Acc 0.9083 | Val Loss 3.0480 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00427 | Train Loss 0.3102 | Train Acc 0.8833 | Val Loss 3.0690 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00428 | Train Loss 0.2901 | Train Acc 0.8917 | Val Loss 3.0878 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00429 | Train Loss 0.2653 | Train Acc 0.8667 | Val Loss 3.1052 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00430 | Train Loss 0.2753 | Train Acc 0.9000 | Val Loss 3.1212 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00431 | Train Loss 0.2665 | Train Acc 0.9000 | Val Loss 3.1353 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00432 | Train Loss 0.2680 | Train Acc 0.9167 | Val Loss 3.1437 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00433 | Train Loss 0.2466 | Train Acc 0.9333 | Val Loss 3.1491 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00434 | Train Loss 0.2934 | Train Acc 0.8833 | Val Loss 3.1549 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00435 | Train Loss 0.2427 | Train Acc 0.9333 | Val Loss 3.1580 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00436 | Train Loss 0.3769 | Train Acc 0.8167 | Val Loss 3.1530 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00437 | Train Loss 0.3415 | Train Acc 0.9000 | Val Loss 3.1441 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00438 | Train Loss 0.3114 | Train Acc 0.8917 | Val Loss 3.1330 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00439 | Train Loss 0.2633 | Train Acc 0.9083 | Val Loss 3.1220 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00440 | Train Loss 0.2601 | Train Acc 0.9167 | Val Loss 3.1113 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00441 | Train Loss 0.2994 | Train Acc 0.9000 | Val Loss 3.1015 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00442 | Train Loss 0.2858 | Train Acc 0.8750 | Val Loss 3.0917 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00443 | Train Loss 0.2827 | Train Acc 0.8833 | Val Loss 3.0831 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00444 | Train Loss 0.2962 | Train Acc 0.8917 | Val Loss 3.0736 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00445 | Train Loss 0.2911 | Train Acc 0.9000 | Val Loss 3.0638 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00446 | Train Loss 0.2867 | Train Acc 0.8750 | Val Loss 3.0561 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00447 | Train Loss 0.2389 | Train Acc 0.9167 | Val Loss 3.0515 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00448 | Train Loss 0.3879 | Train Acc 0.8333 | Val Loss 3.0499 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00449 | Train Loss 0.3019 | Train Acc 0.9000 | Val Loss 3.0462 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00450 | Train Loss 0.2741 | Train Acc 0.9083 | Val Loss 3.0452 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00451 | Train Loss 0.3135 | Train Acc 0.8583 | Val Loss 3.0436 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00452 | Train Loss 0.3103 | Train Acc 0.8750 | Val Loss 3.0440 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00453 | Train Loss 0.2391 | Train Acc 0.9167 | Val Loss 3.0447 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00454 | Train Loss 0.3140 | Train Acc 0.9000 | Val Loss 3.0519 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00455 | Train Loss 0.2715 | Train Acc 0.9167 | Val Loss 3.0623 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00456 | Train Loss 0.2984 | Train Acc 0.9083 | Val Loss 3.0688 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00457 | Train Loss 0.2933 | Train Acc 0.8833 | Val Loss 3.0747 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00458 | Train Loss 0.2789 | Train Acc 0.9167 | Val Loss 3.0790 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00459 | Train Loss 0.3032 | Train Acc 0.8833 | Val Loss 3.0803 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00460 | Train Loss 0.3014 | Train Acc 0.8833 | Val Loss 3.0777 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00461 | Train Loss 0.2962 | Train Acc 0.9000 | Val Loss 3.0801 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00462 | Train Loss 0.2395 | Train Acc 0.9000 | Val Loss 3.0817 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00463 | Train Loss 0.3038 | Train Acc 0.8833 | Val Loss 3.0810 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00464 | Train Loss 0.2443 | Train Acc 0.9000 | Val Loss 3.0811 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00465 | Train Loss 0.3127 | Train Acc 0.9000 | Val Loss 3.0850 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00466 | Train Loss 0.2673 | Train Acc 0.8917 | Val Loss 3.0879 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00467 | Train Loss 0.2858 | Train Acc 0.8917 | Val Loss 3.0899 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00468 | Train Loss 0.3169 | Train Acc 0.9083 | Val Loss 3.0932 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00469 | Train Loss 0.2436 | Train Acc 0.9167 | Val Loss 3.0949 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00470 | Train Loss 0.2819 | Train Acc 0.9083 | Val Loss 3.0954 | Val Acc 0.4375 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00471 | Train Loss 0.2867 | Train Acc 0.8917 | Val Loss 3.0962 | Val Acc 0.4375 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00472 | Train Loss 0.2802 | Train Acc 0.9000 | Val Loss 3.0944 | Val Acc 0.4375 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00473 | Train Loss 0.2749 | Train Acc 0.9167 | Val Loss 3.0930 | Val Acc 0.4375 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00474 | Train Loss 0.2638 | Train Acc 0.9000 | Val Loss 3.0906 | Val Acc 0.4375 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00475 | Train Loss 0.2752 | Train Acc 0.9083 | Val Loss 3.0869 | Val Acc 0.4250 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00476 | Train Loss 0.3282 | Train Acc 0.8667 | Val Loss 3.0842 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00477 | Train Loss 0.2584 | Train Acc 0.9000 | Val Loss 3.0817 | Val Acc 0.4250 | Time(s) 0.0183\n",
            "\n",
            "Epoch 00478 | Train Loss 0.2857 | Train Acc 0.8833 | Val Loss 3.0776 | Val Acc 0.4250 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00479 | Train Loss 0.3365 | Train Acc 0.8417 | Val Loss 3.0763 | Val Acc 0.4250 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00480 | Train Loss 0.2995 | Train Acc 0.8833 | Val Loss 3.0765 | Val Acc 0.4250 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00481 | Train Loss 0.2949 | Train Acc 0.8583 | Val Loss 3.0800 | Val Acc 0.4250 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00482 | Train Loss 0.2656 | Train Acc 0.9000 | Val Loss 3.0830 | Val Acc 0.4250 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00483 | Train Loss 0.2994 | Train Acc 0.9000 | Val Loss 3.0861 | Val Acc 0.4375 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00484 | Train Loss 0.2657 | Train Acc 0.8917 | Val Loss 3.0898 | Val Acc 0.4250 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00485 | Train Loss 0.2546 | Train Acc 0.9417 | Val Loss 3.0960 | Val Acc 0.4250 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00486 | Train Loss 0.2809 | Train Acc 0.9083 | Val Loss 3.1046 | Val Acc 0.4250 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00487 | Train Loss 0.3401 | Train Acc 0.8583 | Val Loss 3.1103 | Val Acc 0.4125 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00488 | Train Loss 0.3558 | Train Acc 0.8333 | Val Loss 3.1144 | Val Acc 0.4125 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00489 | Train Loss 0.2970 | Train Acc 0.9083 | Val Loss 3.1173 | Val Acc 0.4125 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00490 | Train Loss 0.3193 | Train Acc 0.8833 | Val Loss 3.1214 | Val Acc 0.4125 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00491 | Train Loss 0.2449 | Train Acc 0.9083 | Val Loss 3.1255 | Val Acc 0.4125 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00492 | Train Loss 0.2959 | Train Acc 0.8750 | Val Loss 3.1245 | Val Acc 0.4125 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00493 | Train Loss 0.2552 | Train Acc 0.9250 | Val Loss 3.1266 | Val Acc 0.4125 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00494 | Train Loss 0.2880 | Train Acc 0.9000 | Val Loss 3.1304 | Val Acc 0.4125 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00495 | Train Loss 0.2571 | Train Acc 0.9500 | Val Loss 3.1303 | Val Acc 0.4125 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00496 | Train Loss 0.2918 | Train Acc 0.9167 | Val Loss 3.1294 | Val Acc 0.4125 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00497 | Train Loss 0.2574 | Train Acc 0.8917 | Val Loss 3.1283 | Val Acc 0.4125 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00498 | Train Loss 0.3044 | Train Acc 0.8833 | Val Loss 3.1234 | Val Acc 0.4250 | Time(s) 0.0182\n",
            "\n",
            "Epoch 00499 | Train Loss 0.2473 | Train Acc 0.9333 | Val Loss 3.1188 | Val Acc 0.4250 | Time(s) 0.0182\n",
            "\n",
            "9.196543126351932\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}